{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Llama 2 as OCR Vision AI\n",
    "---\n",
    "\n",
    "    Author: Amit Shukla\n",
    "\n",
    "[https://github.com/AmitXShukla](https://github.com/AmitXShukla)\n",
    "\n",
    "[https://twitter.com/ashuklax](https://github.com/AShuklaX)\n",
    "\n",
    "[https://youtube.com/@Amit.Shukla](https://youtube.com/@Amit.Shukla)\n",
    "\n",
    "\n",
    "Meta has recently released Llama 2, a large language model trained with up to 70B parameters, positioning it as the fastest and most advanced solution available. This model is expected to outperform other tools in terms of both speed and accuracy.\n",
    "\n",
    "In this blog post, I will demonstrate some automation use cases I have been working on. \n",
    "\n",
    "It's important to note that these use cases/models will work best when trained on \"in-house\" data. However, training such models is a rigorous task that requires significant computing hours and resources.\n",
    "\n",
    "To make things more accessible and easier to utilize in production, using \"off the shelf\", language models like ChatGPT and Llama 2 is a viable solution.\n",
    "\n",
    "Below, I present some examples of use cases I've been working on. \n",
    "\n",
    "*While these examples are not meant for production*, they still showcase the powerful capabilities of the language models.\n",
    "\n",
    "`Upon completing this blog, you will acquire the skills to build Llama 2 and ChatGPT APIs and harness the capabilities of large language models for practical data analytics tasks.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "---\n",
    "\n",
    "- Introduction\n",
    "- [Llama 2 Installation Windows/Linux](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-UseCases.ipynb)\n",
    "- [Efficient Time and Expense Monitoring with Llama 2](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-Efficient%20Time%20and%20Expense%20Monitoring%20with%20Llama%202.ipynb)\n",
    "- [Using Llama 2 as OCR Vision AI](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-Using%20Llama%202%20as%20OCR%20Vision%20AI.ipynb)\n",
    "- [Llama 2 as Supply Chain Assistant](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-as%20Supply%20Chain%20assistant.ipynb)\n",
    "    - Streamlining 3-Way Receipt Match and Duplicate Voucher Invoices with Llama 2\n",
    "    - Enhancing Fraud Detection: Utilizing Llama 2 as an Advanced Alert System for Monitoring Transactions\n",
    "    - Maximizing Tax Savings, Ensuring Compliance, and Streamlining Audits with Llama 2\n",
    "-  `**WIP**`: Tax Analytics | Spend Classification | Contracts management\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About me\n",
    "I'm Amit Shukla, and I specialize in training neural networks for Finance Supply Chain analysis, enabling them to identify data patterns and make accurate predictions.\n",
    "\n",
    "During the challenges posed by the COVID-19 pandemic, I successfully trained GL and Supply Chain neural networks to anticipate supply chain shortages. The valuable insights gained from this effort have significantly influenced the content of this tutorial series.\n",
    "\t\n",
    "#### Objective:\n",
    "By delving into this powerful tool, we will master the fundamental techniques of utilizing large language models to predict hazards. \n",
    "This knowledge is crucial in preparing finance and supply chain data for advanced analytics, visualization, and predictive modeling using neural networks and machine learning.\n",
    "\t\n",
    "#### Subject\n",
    "It is crucial to emphasize that this specific series will focus exclusively on presenting `production-like examples that demonstrate certain use cases`. It is not intended for production applications. \n",
    "\n",
    "Nevertheless, these examples illustrate highly potent techniques that have practical applications in real-world Data Analytics.\n",
    "\t\n",
    "#### Following\n",
    "In future installments, we will explore Data Analytics and delve into the realm of Data Analytics and machine learning for predictive analytics.\n",
    "\n",
    "Thank you for joining me, and I'm excited to embark on this educational journey together.\n",
    "\t\n",
    "Let's get started.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous video, I demonstrated the process of activating the Open ChatGPT and Llama environments. \n",
    "\n",
    "In this section, I will guide you through the steps to install Llama 2 on a Windows operating system. \n",
    "\n",
    "While the installation process is quite similar to that on Linux, there are a few minor changes that need to be considered. \n",
    "\n",
    "Let's get started!\n",
    "\n",
    "- Step 1: `download miniconda windows installer` [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)\n",
    "- Step 2: create a new conda environment (say llamaConda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before you setup your machine for llama 2, check if you have cuda on your machine\n",
    "\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "# if you don't have cuda and torch on your machine, please move to next step and download cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pytorch cuda\n",
    "# https://pytorch.org/get-started/locally/\n",
    "# uncomment and run this command in Terminal to monitor download progress and debug any error\n",
    "\n",
    "# !conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run these command again and make sure you have cuda available on your machine\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.is_available(),torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this case if you see CUDA out of memory error\n",
    "# also, try to reduce your << --max_batch_size 1 >>, max_split_size_mb:512 and work with \"lowest memory size\" model\n",
    "# !torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b --tokenizer_path tokenizer.model --max_seq_len 512 --max_batch_size 1\n",
    "# clear cache\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:24\"\n",
    "\n",
    "# import gc\n",
    "# del variables\n",
    "# gc.collect()\n",
    "\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "signup and receive model download link from meta.\n",
    "\n",
    "[Mete AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n",
    ">>>\n",
    "    do not use the 'Copy link address' option when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: uncomment and run this command on your machine\n",
    "# make sure, you have a Git installed on your machine if not\n",
    "# for linux run \n",
    "# sudo apt install git-all\n",
    "\n",
    "# for windows, download git from this link\n",
    "# https://git-scm.com/download/win\n",
    "\n",
    "####### clone Meta llama repo ##########\n",
    "git clone https://github.com/facebookresearch/llama.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browse to root of your llama repo\n",
    "\n",
    "## LINUX\n",
    "# cd llama\n",
    "# chmod +x # ./download.sh\n",
    "# ./download.sh\n",
    "\n",
    "## WINDOWS\n",
    "# bash ./download.sh\n",
    "# if this commands error out, download wget.exe from below link and copy wget.ex to C:\\amit.la\\llama\n",
    "# https://eternallybored.org/misc/wget/\n",
    "# make sure, you include << C:\\amit.la\\llama >> to windows environment path so that windows can find it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure, latest conda env is selected as kernel\n",
    "# before activating and installing dependencies\n",
    "\n",
    "!pip install -e .\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for windows\n",
    "# change line #62 on llama/generate.py\n",
    "# from \n",
    "# << torch.distributed.init_process_group(\"gloo|nccl\") >>\n",
    "# to\n",
    "# << torch.distributed.init_process_group(\"gloo|nccl\") >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure, latest conda env (llamaConda) is selected as kernel\n",
    "# before activating and installing dependencies\n",
    "\n",
    "# !pip install -e .\n",
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./example_text_completion.py\n",
    "# ./example_chat_completion.py\n",
    "\n",
    "# change prompts | dialogue \n",
    "#   prompts = [\n",
    "#         # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "#         \"meaning of life is\",\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are ready to use llama\n",
    "# !torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b --tokenizer_path tokenizer.model --max_seq_len 512 --max_batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addressing Llama | Cuda errors\n",
    "\n",
    "---\n",
    "\n",
    "Here is another solution proposed by one of users who was able to use Llama on CPU machines.\n",
    "\n",
    "** `Please see, If you successfully install this using this approach, please open an Issue at this GitHub repository so that I can update notes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install https://github.com/krychu/llama instead of https://github.com/facebookresearch/llama\n",
    "\n",
    "# execute download.sh in a new terminal, provide META AI URL and download 7B model and model weights\n",
    "#   run the download.sh script in a terminal, passing the URL provided when prompted to start the download\n",
    "# create a new env\n",
    "\n",
    "# python3 -m venv env\n",
    "# source env/bin/activate\n",
    "\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu #pour la version cpu\n",
    "# python3 -m pip install -e .\n",
    "\n",
    "\n",
    "# torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b/ --tokenizer_path tokenizer.model --max_seq_len 128 --max_batch_size 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama 2 as OCR Vision AI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What incentive do I have to develop yet another OCR Vision AI tool when there are already thousands of options readily available in market?`\n",
    "\n",
    "- The majority of the offerings in the market or available wrappers are built upon Open Source OCR packages.\n",
    "- In contrast, Vision AI solutions by large companies, which are not reliant on Open Source technologies, often offer low upfront costs but impose significant charges later on, frequently requiring customers to pay per image used.\n",
    "\n",
    "`Why pay for inferior results from a service not trained on your data, when building an in-house, cost-effective, on-premise solution with customized capabilities is feasible, without risking data exposure.`\n",
    "\n",
    "Here are some use cases you can consider optimizing this OCR Vision API by training it on \"in-house\" data.\n",
    "\n",
    "1. Document classifier\n",
    "2. Digital Invoice scanner\n",
    "3. Digital Private signature matching\n",
    "4. Scanning for confidential information like PHI, Private health, or Personal data\n",
    "5. Train on \"in-house\" data for classifying \"Secured information\" in contracts, spends, etc.\n",
    "7. Label or Hash documents\n",
    "8. Sorting Receipts, Vendor Invoices or Matching Employee Expenses\n",
    "9. Duplicate Invoice\n",
    "....\n",
    "\n",
    "\n",
    "This OCR Vision AI code below, is just one of many ways while numerous smarter optimizations are possible.\n",
    "and `Fine tuning these models with in-house knowledge base will results into more accurate results.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR, Vision AI automated build process\n",
    "---\n",
    "\n",
    "Step 1: A receipt or invoice document is uploaded / dropped to a folder\n",
    "       or you system takes a screenshot of an image or document\n",
    "\n",
    "Step 2: Automated Code to call script is executed as soon as file is dropped\n",
    "\n",
    "Step 3: Scripts read text from images\n",
    "\n",
    "Step 4: Scripts to build prompts\n",
    "\n",
    "Step 5: Send prompts to Llama 2 | ChatGPT\n",
    "\n",
    "Step 6: Store results back to Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Step 1: System takes a screenshot of a webpage\n",
    "#########################################################################\n",
    "# pip install Pillow\n",
    "# pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from PIL import Image\n",
    "\n",
    "# Define the URL of the web page we want to screenshot\n",
    "\n",
    "url = 'https://finance.yahoo.com/quote/AAPL?p=AAPL&.tsrc=fin-srch'\n",
    "\n",
    "# Define the path to the webdriver executable (e.g., chromedriver.exe)\n",
    "\n",
    "# webdriver_path = '/path/to/webdriver/executable'\n",
    "webdriver_path = 'C:\\amit.la\\WIP\\RPA\\downloads\\chromedriver.exe'\n",
    "\n",
    "# Set up the webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True # Run the browser in headless mode to prevent a window from popping up\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path, options=options)\n",
    "\n",
    "# Load the web page\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Take a screenshot of the entire page\n",
    "\n",
    "# screenshot = driver.find_element_by_tag_name('body').screenshot_as_png\n",
    "screenshot = driver.save_screenshot('../downloads/screenshot.png')\n",
    "\n",
    "# Close the webdriver\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the screenshot to a file\n",
    "\n",
    "# with open('../SampleData/screenshot.png', 'wb') as file:\n",
    "#     file.write(screenshot)\n",
    "\n",
    "# Open the screenshot with Pillow to display it (optional)\n",
    "\n",
    "img = Image.open('../downloads/screenshot.png')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Step 1: A receipt or invoice document is uploaded / dropped to a folder\n",
    "#########################################################################\n",
    "\n",
    "#   $ sftp remote_username@server_ip_or_hostname\n",
    "# >> Connected to remote_username@server_ip_or_hostname.\n",
    "\n",
    "# $ sftp> pwd\n",
    "# >> Remote working directory: /home/remote_username\n",
    "\n",
    "# $ sftp> lpwd\n",
    "# >> Local working directory: /downloads\n",
    "\n",
    "# $ sftp> put screenshot.png\n",
    "\n",
    "\n",
    "## TAKING A SCREENSHOT ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Step 2: Automated Code to call script is executed as soon as file is dropped\n",
    "##############################################################################\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Define the directory to watch\n",
    "\n",
    "watch_dir = '/downloads/AAPL.png'\n",
    "\n",
    "# Define the command to run when a new file is added\n",
    "\n",
    "command = 'python c:/amit.la/WIP/RPA/ocr.py'\n",
    "\n",
    "# Define the set of already seen files\n",
    "\n",
    "seen_files = set()\n",
    "\n",
    "# Start watching the directory for new files\n",
    "\n",
    "while True:\n",
    "    # Get the list of files in the directory\n",
    "    files = os.listdir(watch_dir)\n",
    "\n",
    "    # Check each file in the directory\n",
    "    for file in files:\n",
    "        # If the file is new, trigger the command\n",
    "        if file not in seen_files:\n",
    "            seen_files.add(file)\n",
    "            os.system(command) # add to your DB...\n",
    "\n",
    "    # Wait for a little bit before checking again\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Step 3: Scripts read text from images\n",
    "#######################################\n",
    "# py -m pip install pytesseract : open source\n",
    "# py -m pip install PIL\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "##############################################################################\n",
    "# in case if tesseract is not included in PATH\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\amit.la\\WIP\\RPA\\downloads\\ts\\tesseract.exe'\n",
    "##############################################################################\n",
    "\n",
    "def read_image_text(image_path):\n",
    "    \"\"\"\n",
    "    Reads text from an image file using Tesseract OCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The file path to the input image.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text from the image.\n",
    "    \"\"\"\n",
    "    # Load the image file\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Use Tesseract OCR to extract the text from the image\n",
    "    text = pytesseract.image_to_string(image)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "image_path = \"../downloads/AAPL.png\"\n",
    "# image_path = \"../downloads/medical_form.png\"\n",
    "# image_path = \"../downloads/email.png\"\n",
    "# image_path = \"../downloads/vaccine.png\"\n",
    "# image_path = \"../downloads/blurry_1.png\"\n",
    "# image_path = \"../downloads/blurry_2.png\"\n",
    "text = read_image_text(image_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Step 4: Scripts to build prompts\n",
    "#######################################\n",
    "\n",
    "promptText = read_image_text(image_path)\n",
    "print(promptText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Step 5: Send prompts to Llama 2 | ChatGPT\n",
    "###########################################\n",
    "\n",
    "def callChatGPT(prompt):\n",
    "    ###########################################\n",
    "    # uncomment this code to call ChatGPT API #\n",
    "    ###########################################\n",
    "    # completion = openai.ChatCompletion.create(\n",
    "    # model=model_engine,\n",
    "    # messages=[\n",
    "    #     {\"role\": \"user\", \"content\": prompt}\n",
    "    # ])\n",
    "    # return completion.choices[0].message.content\n",
    "    return \"Yes\"\n",
    "\n",
    "def callLlama(prompt):\n",
    "    ###########################################\n",
    "    # uncomment this code to call Llama API #\n",
    "    ###########################################\n",
    "    # results = generator.chat_completion(\n",
    "    #     dialogs,  # type: ignore\n",
    "    #     max_gen_len=max_gen_len,\n",
    "    #     temperature=temperature,\n",
    "    #     top_p=top_p,\n",
    "    # )\n",
    "    # return result['generation']['content']\n",
    "    return \"Yes\"\n",
    "\n",
    "# build dynamic prompt per use case\n",
    "\n",
    "\n",
    "prompt = \"respond in one word: average volume of stock in this text.\"\n",
    "prompt += promptText    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(promptText)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you've provided some financial data related to the stock of Apple Inc. (AAPL). Here's a breakdown of the information:\n",
      "\n",
      "    Stock Symbol: AAPL (Apple Inc.)\n",
      "    Current Stock Price: $159.74\n",
      "        Change Today: +$0.46 (+0.29%)\n",
      "    Market Cap: $252.71 billion\n",
      "    Previous Close: $159.28\n",
      "    Day's Range: $159.12 - $160.51\n",
      "    52-Week Range: $124.27 - $179.61\n",
      "    Volume: 23,782,692 shares\n",
      "    Average Volume: 70,701,586 shares\n",
      "    Beta: Not provided in the snippet\n",
      "    PE Ratio (TTM - Trailing Twelve Months): 160.15x800\n",
      "    EPS (TTM - Trailing Twelve Months): $5.89\n",
      "    Earnings Date: April 26, 2023\n",
      "    Ex-Dividend Date: February 10, 2023\n",
      "    Dividend (Annual): Not provided in the snippet\n",
      "    Forward P/E Ratio: Not provided in the snippet\n",
      "    Target Estimate: $169.23\n",
      "\n",
      "Please note that this data appears to be from April 26, 2023, and market conditions may have changed since then. It's always a good idea to verify the latest data through reliable financial sources before making any investment decisions.\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# Step 6: Store results back to Application\n",
    "###########################################\n",
    "# res2 = callLlama(promptText)\n",
    "# res = callChatGPT(prompt)\n",
    "# res2 = callLlama(promptText)\n",
    "# res = 70,701,586\n",
    "print(res2)\n",
    "\n",
    "# respon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The use case discussed above exemplify sophisticated business processes and there is certainly lot more which is not covered. \n",
    "\n",
    "This use case merely scratch the surface of what can be achieved with these advanced tools. \n",
    "\n",
    "You may argue that the same results can be attained using simple algebraic mathematics with these datasets, and I fully support and agree with this observation.\n",
    "\n",
    "In essence, the entire field, encompassing Data Science, Python, Llama, and ChatGPT, revolves around uncovering statistical associations within data.\n",
    "\n",
    "However, it is crucial to recognize that the deployment of Llama or ChatGPT-like models does not surpass the importance of traditional statistics,\n",
    "instead, they should be employed to streamline specific tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
