{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for Pro Data Analyst\n",
    "\n",
    "A Comprehensive Guide to Exploratory Data Analysis with Real-Life Data, Transforming Beginners into Professionals.\n",
    "\n",
    "---\n",
    "[Download PDF version of this notebook](https://github.com/AmitXShukla/RPA/blob/main/SampleData/The%20Ultimate%20Guide%20to%20Data%20Wrangling%20with%20Python%20-%20Rust%20Polars%20Data%20Frame.pdf)\n",
    "\n",
    "[Video Tutorials](https://www.youtube.com/playlist?list=PLp0TENYyY8lHJaY4t5bAihnFS5TBUQYV1)\n",
    "\n",
    "    Author: Amit Shukla\n",
    "\n",
    "[https://github.com/AmitXShukla](https://github.com/AmitXShukla)\n",
    "\n",
    "[https://twitter.com/ashuklax](https://github.com/AShuklaX)\n",
    "\n",
    "[https://youtube.com/AmitXShukla](https://youtube.com/@Amit.Shukla)\n",
    "\n",
    "by the end of this blog, you will learn techniques to\n",
    "\n",
    "- Data Discovery using Pandas 2.0\n",
    "- Create Data ERD diagram with animation (using manim)\n",
    "- Data Visualization using Matplotlib\n",
    "- Data Visualization using PlotLy\n",
    "- Data Visualization using Seaborn\n",
    "- Analyze Distributions\n",
    "- Spot Anomalies\n",
    "- Test Hypothesis\n",
    "- Data Patterns\n",
    "- Check Assumptions\n",
    "- Create Interactive Visualizations\n",
    "- what-if Analysis\n",
    "- would, could, should\n",
    "- Time Travel on Time Series Data\n",
    "- Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "#### Introduction\n",
    "I'm Amit Shukla, and I specialize in training neural networks for finance supply chain analysis, enabling them to identify data patterns and make accurate predictions.\n",
    "During the challenges posed by the COVID-19 pandemic, I successfully trained GL and Supply Chain neural networks to anticipate supply chain shortages. The valuable insights gained from this effort have significantly influenced the content of this tutorial series.\n",
    "\t\n",
    "#### Objective:\n",
    "By delving into this powerful tool, we will master the fundamental techniques of using Exploratory Data Analysis. This knowledge is crucial in preparing finance and supply chain data for advanced analytics, visualization, and predictive modeling using neural networks and machine learning.\n",
    "\t\n",
    "#### Subject\n",
    "It's important to note that this particular series will concentrate solely on `Exploratory Data Analysis`.\n",
    "\t\n",
    "#### Following\n",
    "However, in future installments, we will explore Data Analytics and delve into the realm of machine learning for predictive analytics.\n",
    "\tThank you for joining me, and I'm excited to embark on this educational journey together.\n",
    "\t\n",
    "Let's get started.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "---\n",
    "\n",
    "- What is EDA\n",
    "- Installation\n",
    "- Loading Finance and Supply chain Data\n",
    "- Data Discovery using Pandas 2.0\n",
    "- Create Data ERD diagram with animation (using manim)\n",
    "- Data Visualization using Matplotlib\n",
    "- Data Visualization using PlotLy\n",
    "- Data Visualization using Seaborn\n",
    "- Analyze Distributions\n",
    "- Spot Anomalies\n",
    "- Test Hypothesis\n",
    "- Data Patterns\n",
    "- Check Assumptions\n",
    "- Create Interactive Visualizations\n",
    "- what-if Analysis\n",
    "- would, could, should\n",
    "- Time Travel on Time Series Data\n",
    "- Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is EDA\n",
    "---\n",
    "EDA is often characterized as a tool for data scientists to discover patterns, spot anomalies, test a hypothesis, or check assumptions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install polars pandas numpy matplotlib seaborn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Finance and Supply chain Data\n",
    "---\n",
    "\n",
    "using this section, we will first load our dataset.\n",
    "\n",
    "It's important for user to make sure, that all steps discussed in this section run without any error and data is loaded before starting our data discovery journey on EDA.\n",
    "\n",
    "Please see, I want user to ignore technical content of this section for now, as this section is only required to load data.\n",
    "How this data is loaded is not the point, point is, this excercise is about using EDA on a real life dataset.\n",
    "\n",
    "As we progress more, in later sections, I want users to use EDA to discover data patterns and confirm those findings with dataset created in this section, this is why technical details used to create these datasets are intentionally ignored for now.\n",
    "\n",
    "Let's load this data now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Discovery\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial stage of Data Discovery, the primary step involves recognizing and establishing a dynamic repository that encompasses all accessible datasets. It is imperative to identify the relationships between these datasets before embarking on data transformation or analytics.\n",
    "\n",
    "This phase is of utmost importance, as it entails creating an official diagram reminiscent of an Entity-Relationship Diagram (ERD). The crucial tasks include pinpointing data types and discerning the fields that contain valuable information. This not only aids in comprehending the data but also facilitates a deeper understanding of the business processes or the insights derived from these datasets.\n",
    "\n",
    "In this section, we will delve into the Data Discovery phase. We will initiate the process by scrutinizing the available data and crafting an ERD that encapsulates the dataset structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by examining the available dataset.\n",
    "\n",
    "For now, we won't concern ourselves with its source, I'll provide the scripts used to generate it later. \n",
    "\n",
    "Our goal is to simulate a real-world project scenario where analysts often receive unfamiliar datasets and initiate data exploration.\n",
    "\n",
    "The following steps demonstrate this process, and we'll take it one step at a time to learn how to approach data discovery. \n",
    "\n",
    "Keep in mind that there's no one-size-fits-all approach, it varies based on data types and quality. \n",
    "\n",
    "Consider these steps as general guidelines. Let's begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset is generated using Polars DataFrame\n",
    "# make sure you have polars installed before generating this dataset\n",
    "\n",
    "pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accounts.csv', 'dept.csv', 'earth.jpg', 'ledger.csv', 'ledger.json', 'ledger.parquet', 'location.csv']\n",
      "['accounts.csv', 'dept.csv', 'earth.jpg', 'ledger.csv', 'ledger.json', 'ledger.parquet', 'location.csv']\n"
     ]
    }
   ],
   "source": [
    "# use this script to create sample Finance dataset\n",
    "\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "dirPath = \"../../../downloads/\" # directory where sample csv are generated\n",
    "sampleSize = 100_000 # generate 100k sample rows\n",
    "\n",
    "print(os.listdir(dirPath))\n",
    "\n",
    "# Creating DataFrame from a dict or a collection of dicts.\n",
    "# let's create a more sophisticated DataFrame\n",
    "# in real world, Organization maintain dozens of record structure to store \n",
    "# different type of locations, like ShipTo Location, Receiving, \n",
    "# Mailing, Corp. office, head office,\n",
    "# field office etc. etc.\n",
    "\n",
    "########################\n",
    "## LOCATION DataFrame ##\n",
    "########################\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "location = pl.DataFrame({\n",
    "    \"ID\":  list(range(11, 23)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n",
    "                     \"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n",
    "                     \"Dallas\",\"San Francisco\"],\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\"] * 3,\n",
    "    \"TYPE\" : \"Physical\",\n",
    "    \"CATEGORY\" : [\"Ship\",\"Recv\",\"Mfg\"] * 4\n",
    "})\n",
    "location.sample(5).with_row_count(\"Row #\")\n",
    "\n",
    "########################\n",
    "## ACCOUNTS DataFrame ##\n",
    "########################\n",
    "\n",
    "accounts = pl.DataFrame({\n",
    "    \"ID\":  list(range(10000, 45000, 1000)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Operating Expenses\",\"Non Operating Expenses\",\"Assets\",\n",
    "                     \"Liabilities\",\"Net worth accounts\", \"Statistical Accounts\",\n",
    "                     \"Revenue\"] * 5,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\", \"Region E\"] * 7,\n",
    "    \"TYPE\" : [\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \n",
    "                        \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\n",
    "                        \"REVENUE\"] * 5,\n",
    "    \"CATEGORY\" : [\n",
    "       \t\t\"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n",
    "       \t\t\"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n",
    "       \t\t\"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n",
    "       \t\t\"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n",
    "       \t\t\"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n",
    "       \t\t\"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n",
    "       \t\t\"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"\n",
    "       \t],\n",
    "})\n",
    "accounts.sample(5).with_row_count(\"Row #\")\n",
    "\n",
    "##########################\n",
    "## DEPARTMENT DataFrame ##\n",
    "##########################\n",
    "\n",
    "dept = pl.DataFrame({\n",
    "    \"ID\":  list(range(1000, 2500, 100)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Sales & Marketing\",\"Human Resource\",\n",
    "                     \"Information Technology\",\"Business leaders\",\"other temp\"] * 3,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"] * 3,\n",
    "    \"TYPE\" : [\"S\",\"H\",\"I\",\"B\",\"O\"] * 3,\n",
    "    \"CATEGORY\" : [\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"] * 3,\n",
    "})\n",
    "dept.sample(5).with_row_count(\"Row #\")\n",
    "\n",
    "######################\n",
    "## LEDGER DataFrame ##\n",
    "######################\n",
    "\n",
    "org = \"ABC Inc.\"\n",
    "ledger_type = \"ACTUALS\" # BUDGET, STATS are other Ledger types\n",
    "fiscal_year_from = 2020\n",
    "fiscal_year_to = 2023\n",
    "random.seed(123)\n",
    "\n",
    "ledger = pl.DataFrame({\n",
    "\t\"LEDGER\" : ledger_type,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, \n",
    "                                          fiscal_year_to+1, 1)),k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledger.sample(5).with_row_count(\"Row #\")\n",
    "\n",
    "ledger_type = \"BUDGET\" # ACTUALS, STATS are other Ledger types\n",
    "\n",
    "ledgerBudget = pl.DataFrame({\n",
    "\t\"LEDGER\" : ledger_type,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, fiscal_year_to+1, 1))\n",
    "                               ,k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledgerBudget.sample(5).with_row_count(\"Row #\")\n",
    "#########################################\n",
    "# combined ledger for Actuals and Budget\n",
    "#########################################\n",
    "dfLedger = pl.concat([ledger, ledger_budg], how=\"vertical\")\n",
    "dfLedger.sample(5).with_row_count(\"Row #\")\n",
    "\n",
    "location.write_csv(f\"{dirPath}location.csv\")\n",
    "dept.write_csv(f\"{dirPath}dept.csv\")\n",
    "accounts.write_csv(f\"{dirPath}accounts.csv\")\n",
    "dfLedger.write_csv(f\"{dirPath}ledger.csv\")\n",
    "\n",
    "print(os.listdir(dirPath))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas to load data into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible to read into DataFrame:  accounts.csv\n",
      "Eligible to read into DataFrame:  dept.csv\n",
      "Eligible to read into DataFrame:  ledger.csv\n",
      "Eligible to read into DataFrame:  location.csv\n",
      "(35, 8) (15, 8) (12, 6) (200000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEDGER</th>\n",
       "      <th>ORG</th>\n",
       "      <th>FISCAL_YEAR</th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>ACCOUNT</th>\n",
       "      <th>DEPT</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>POSTED_TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16322</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>39000</td>\n",
       "      <td>2400</td>\n",
       "      <td>20</td>\n",
       "      <td>919779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41713</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>27000</td>\n",
       "      <td>1800</td>\n",
       "      <td>22</td>\n",
       "      <td>282452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94363</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>44000</td>\n",
       "      <td>2000</td>\n",
       "      <td>16</td>\n",
       "      <td>622392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179527</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>35000</td>\n",
       "      <td>1300</td>\n",
       "      <td>21</td>\n",
       "      <td>739302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176177</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>39000</td>\n",
       "      <td>1200</td>\n",
       "      <td>17</td>\n",
       "      <td>573450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37782</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>23000</td>\n",
       "      <td>1300</td>\n",
       "      <td>22</td>\n",
       "      <td>51061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108868</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>38000</td>\n",
       "      <td>1300</td>\n",
       "      <td>18</td>\n",
       "      <td>500272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112988</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>17000</td>\n",
       "      <td>1700</td>\n",
       "      <td>16</td>\n",
       "      <td>102737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39437</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>15000</td>\n",
       "      <td>1000</td>\n",
       "      <td>13</td>\n",
       "      <td>923997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72200</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>42000</td>\n",
       "      <td>2400</td>\n",
       "      <td>22</td>\n",
       "      <td>417441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LEDGER       ORG  FISCAL_YEAR  PERIOD  ACCOUNT  DEPT  LOCATION  \\\n",
       "16322   ACTUALS  ABC Inc.         2022       5    39000  2400        20   \n",
       "41713   ACTUALS  ABC Inc.         2021      10    27000  1800        22   \n",
       "94363   ACTUALS  ABC Inc.         2020       5    44000  2000        16   \n",
       "179527   BUDGET  ABC Inc.         2020       7    35000  1300        21   \n",
       "176177   BUDGET  ABC Inc.         2022      12    39000  1200        17   \n",
       "37782   ACTUALS  ABC Inc.         2022      11    23000  1300        22   \n",
       "108868   BUDGET  ABC Inc.         2021       3    38000  1300        18   \n",
       "112988   BUDGET  ABC Inc.         2022       7    17000  1700        16   \n",
       "39437   ACTUALS  ABC Inc.         2021      11    15000  1000        13   \n",
       "72200   ACTUALS  ABC Inc.         2023       2    42000  2400        22   \n",
       "\n",
       "        POSTED_TOTAL  \n",
       "16322         919779  \n",
       "41713         282452  \n",
       "94363         622392  \n",
       "179527        739302  \n",
       "176177        573450  \n",
       "37782          51061  \n",
       "108868        500272  \n",
       "112988        102737  \n",
       "39437         923997  \n",
       "72200         417441  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dirPath = \"../../../downloads/\"\n",
    "# print(os.listdir(dirPath))\n",
    "\n",
    "# as you can see, there are many files,\n",
    "# we will focus on loading only csv/xls/xlsx files for now\n",
    "\n",
    "for filename in os.listdir(dirPath):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(\"Eligible to read into DataFrame: \", filename)\n",
    "\n",
    "import pandas as pd\n",
    "dfAccounts = pd.read_csv(dirPath+\"accounts.csv\")\n",
    "dfDept = pd.read_csv(dirPath+\"dept.csv\")\n",
    "dfLocation = pd.read_csv(dirPath+\"location.csv\")\n",
    "dfLedger = pd.read_csv(dirPath+\"ledger.csv\")\n",
    "\n",
    "print(dfAccounts.shape, dfDept.shape, dfLocation.shape, dfLedger.shape)\n",
    "dfLedger.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.sample of                                              columns\n",
       "0  {'name': 'LEDGER', 'datatype': 'Utf8', 'values...\n",
       "1  {'name': 'ORG', 'datatype': 'Utf8', 'values': ...\n",
       "2  {'name': 'FISCAL_YEAR', 'datatype': 'Int64', '...\n",
       "3  {'name': 'PERIOD', 'datatype': 'Int64', 'value...\n",
       "4  {'name': 'ACCOUNT', 'datatype': 'Int64', 'valu...\n",
       "5  {'name': 'DEPT', 'datatype': 'Int64', 'values'...\n",
       "6  {'name': 'LOCATION', 'datatype': 'Int64', 'val...\n",
       "7  {'name': 'POSTED_TOTAL', 'datatype': 'Int64', ...>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# while discussing IO operations\n",
    "# let's take a quick look at how to read JSON, parquet and excel files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "dirPath = \"../../../downloads/\"\n",
    "os.listdir(dirPath)\n",
    "\n",
    "# read json file\n",
    "dfTemp = pd.read_json(dirPath+\"ledger.json\")\n",
    "\n",
    "# # read parquet file\n",
    "# dfTemp = pd.read_parquet(dirPath+\"ledger.parquet\", engine=\"pyarrow\")\n",
    "# # pip install pyarrow before use\n",
    "# # fastparquet is another engine used\n",
    "\n",
    "# # in case of excel files, use read_excel function\n",
    "# dfTemp = pd.read_excel(dirPath+\"ledger.xls\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# # handy function to read each Excel sheet in a Python dictionary\n",
    "# workbook = pd.ExcelFile('ledger.xlsx')\n",
    "# dictionary = {}\n",
    "# for sheet_name in workbook.sheet_names:\n",
    "# df = workbook.parse(sheet_name)\n",
    "# dictionary[sheet_name] = dfLedger\n",
    "\n",
    "dfTemp.sample\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "dirPath = \"../../../downloads/\"\n",
    "os.listdir(dirPath)\n",
    "\n",
    "# read json file\n",
    "dfTemp = pd.read_json(dirPath+\"ledger.json\")\n",
    "\n",
    "# # read parquet file\n",
    "# dfTemp = pd.read_parquet(dirPath+\"ledger.parquet\", engine=\"pyarrow\")\n",
    "# # pip install pyarrow before use\n",
    "# # fastparquet is another engine used\n",
    "\n",
    "# # in case of excel files, use read_excel function\n",
    "# dfTemp = pd.read_excel(dirPath+\"ledger.xls\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# # handy function to read each Excel sheet in a Python dictionary\n",
    "# workbook = pd.ExcelFile('ledger.xlsx')\n",
    "# dictionary = {}\n",
    "# for sheet_name in workbook.sheet_names:\n",
    "# df = workbook.parse(sheet_name)\n",
    "# dictionary[sheet_name] = dfLedger\n",
    "\n",
    "dfTemp.sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have got a glimpse of Finance datasets,\n",
    "\n",
    "let's learn more about Pandas Data Structures which holds these datasets and allowed us to perform multiple operations.\n",
    "\n",
    "Pandas DataFrame provide two major data structure to work this.\n",
    "\n",
    "#### Series and DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data ERD diagram with animation (using manim)\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization using Matplotlib\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization using PlotLy\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization using Seaborn\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
