{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ultimate Guide to Data Wrangling with Python | Rust Polars DataFrame\n",
    "---\n",
    "\n",
    "    Author: Amit Shukla\n",
    "[https://github.com/AmitXShukla](https://github.com/AmitXShukla)\n",
    "\n",
    "[https://twitter.com/ashuklax](https://github.com/AShuklaX)\n",
    "\n",
    "[https://youtube.com/AmitXShukla](https://youtube.com/@Amit.Shukla)\n",
    "\n",
    "The aim of this comprehensive user guide is to equip you with all the necessary knowledge and skills required to utilize Python Polars DataFrames effectively for financial and supply chain data science analytics. \n",
    "\n",
    "It provides an in-depth overview of the most commonly used functions and capabilities of the package.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "I'm Amit Shukla, and I specialize in training neural networks for finance supply chain analysis, enabling them to identify data patterns and make accurate predictions.\n",
    "During the challenges posed by the COVID-19 pandemic, I successfully trained GL and Supply Chain neural networks to anticipate supply chain shortages. The valuable insights gained from this effort have significantly influenced the content of this tutorial series.\n",
    "\t\n",
    "#### Objective:\n",
    "By delving into this powerful tool, we will master the fundamental techniques of Data Wrangling. This knowledge is crucial in preparing finance and supply chain data for advanced analytics, visualization, and predictive modeling using neural networks and machine learning.\n",
    "\t\n",
    "#### Subject\n",
    "It's important to note that this particular series will concentrate solely on `Data Wrangling`. \n",
    "Throughout this series, our main focus will be on learning `Rust Polars Data Frame Python library`.\n",
    "\t\n",
    "#### Following\n",
    "However, in future installments, we will explore Data Analytics and delve into the realm of machine learning for predictive analytics.\n",
    "\tThank you for joining me, and I'm excited to embark on this educational journey together.\n",
    "\t\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "---\n",
    "\n",
    "- Why another DataFrame\n",
    "- Installation\n",
    "- Finance and Supply chain Data Analytics\n",
    "- Creating Polars DataFrame\n",
    "- Data Types\n",
    "- IO\n",
    "- About Contexts\n",
    "    - selection\n",
    "    - filtering\n",
    "    - group by\n",
    "- About Expressions\n",
    "- Functions, User defined functions and Windows function\n",
    "- Transformation\n",
    "- Polar SQLs\n",
    "- Lazy API | Eager execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why another DataFrame\n",
    "---\n",
    "Despite the numerous state-of-the-art dataframe packages available in the market, the Polar dataframe, which is built on RUST, boasts the fastest execution speed, enabling it to handle complex data science operations on tabular datasets.\n",
    "\n",
    "- Execution on larger-than-memory (RAM) data analytics\n",
    "- Lazy API vs Eager execution\n",
    "- Automatic Optimization\n",
    "- Embarrassingly Parallel\n",
    "- Easy to learn consistent, predictable API that has strict schema\n",
    "- SQLs like expressions\n",
    "\n",
    "#### Efficient Execution of Analytics on Large-than-Memory (RAM) Data\n",
    "\n",
    "RAM is not a big deal these days as most computers and VMs offer inexpensive GBs of RAM. In fact, the availability of affordable RAM is the primary reason why Pandas-like DataFrames remain the go-to choice, and it is unlikely that Pandas or R Tables will become obsolete anytime soon.\n",
    "\n",
    "However, Polars DataFrames are increasingly gaining popularity among developers due to their ability to harness the horsepower of Apache Spark, the backend support of DuckDB and Apache Arrow, and the ease-of-use of Pandas-like data frame functionalities. \n",
    "\n",
    "Additionally, Polars comes with built-in multi-core, multi-threaded parallel processing, making it a highly preferred choice.\n",
    "\n",
    "#### Lazy API vs Eager execution\n",
    "\n",
    "Just because an API is referred to as \"lazy\" does not necessarily imply that there will be a delay in processing or execution, and conversely, \"eager\" execution doesn't necessarily mean that the programming language will process data transformations or begin execution immediately and more quickly.\n",
    "\n",
    "In simpler terms, using a Lazy API implies that the API will first take the time to optimize the query before execution, which often results in improved performance.\n",
    "\n",
    "To illustrate this concept, consider running SQL on an RDBMS database. If the statistics, indexes, and data partitions have been appropriately optimized and the SQL is written in an optimized manner that utilizes the available statistics, indexes, and data partitions, the results will be delivered more quickly.\n",
    "\n",
    "#### Automatic Optimization\n",
    "\n",
    "We will learn few automation techniques to efficiently optimize queries.\n",
    "\n",
    "#### Embarrassingly Parallel\n",
    "\n",
    "#### Easy to learn consistent, predictable API that has strict schema\n",
    "\n",
    "#### SQLs like expressions\n",
    "\n",
    "`Let's get started`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U polars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finance and Supply chain Data Analytics\n",
    "---\n",
    "\n",
    "#### Finance data model\n",
    "\n",
    "A finance data model is a comprehensive and structured framework used to represent and organize financial information within an organization.\n",
    "\n",
    "It serves as the blueprint for how financial data is collected, stored, processed, and analyzed, ensuring accuracy, consistency, and efficiency in managing financial operations. \n",
    "\n",
    "The model defines the relationships between various financial entities such as assets, liabilities, revenues, expenses, and equity, enabling financial professionals to gain insights into the company's financial health, performance, and risk exposure. \n",
    "\n",
    "It typically encompasses multiple dimensions, including time, currency, and geographical locations, chart of accounts, departments / cost centers, fiscal years and reporting accounting periods providing a holistic view of the organization's financial landscape. \n",
    "\n",
    "A well-designed finance data model is critical for generating accurate financial reports, facilitating financial planning and forecasting, and supporting strategic decision-making at all levels of the business.\n",
    "\n",
    "As stated above, since our objective is learn Data Science operations on Finance and Supply chain dataset, we will focus on creating\n",
    "few real life examples which are similar to Finance and Supply chain.\n",
    "\n",
    "For more information, please learn more about [Finance and Supply chain ERP data](https://amitxshukla.github.io/GeneralLedger.jl/tutorials/erd/).\n",
    "\n",
    "Objective of following section is to understand ERP GL like data. \n",
    "\n",
    "A sample of data structure and ERD relationship diagram can be seen in this diagram below.\n",
    "\n",
    "`Finance ER Diagram`\n",
    "\n",
    "![ERD Diagram](https://github.com/AmitXShukla/AmitXShukla.github.io/raw/master/blogs/PlutoCon/gl_erd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supply chain data model\n",
    "\n",
    "A supply chain data model is a structured representation of the various elements and interactions within a supply chain network.\n",
    "\n",
    "It encompasses critical components such as customers, orders, receipts, products, invoices, vouchers, and ship-to locations. \n",
    "\n",
    "Customers form the foundation of the supply chain, as they drive demand for products. Orders and receipts represent the flow of goods and services, capturing the movement of inventory throughout the supply chain. \n",
    "\n",
    "The product entity accounts for the diverse range of items being handled, from raw materials to finished goods.\n",
    "\n",
    "Invoices and vouchers track financial transactions, ensuring transparent and accurate billing processes. \n",
    "\n",
    "Ship-to locations specify the destinations of goods during the distribution process.\n",
    "\n",
    "By establishing relationships and attributes between these elements, the supply chain data model aids in optimizing inventory management, forecasting demand, enhancing order fulfillment, and ultimately, improving overall operational efficiency within the supply chain ecosystem.\n",
    "\n",
    "`Supply Chain ER Diagram`\n",
    "\n",
    "![ERD Diagram](../SampleData/ER_Flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Polars DataFrame\n",
    "---\n",
    "\n",
    "#### Polars Data Structure\n",
    "\n",
    "The core base data structures provided by Polars are Series and DataFrames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finance DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "## Series and DataFrames\n",
    "#########################\n",
    "import polars as pl\n",
    "\n",
    "# with a tuple\n",
    "location_1 = pl.Series([\"CA\", \"OR\", \"WA\", \"TX\", \"NY\"]) \n",
    "# location_1 series when will converted to DataFrame will not have a column name\n",
    "# and hence, later when is used to create a DataFrame will assign a column name like column_xx\n",
    "\n",
    "location_2 = pl.Series(\"location\", [\"CA\", \"OR\", \"WA\", \"TX\", \"NY\"])\n",
    "\n",
    "print(f\"Location Type: Series 1: \", location_1)\n",
    "print(f\"Location Type: Series 2: \", location_2)\n",
    "\n",
    "location_1_df = pl.DataFrame(location_1)\n",
    "location_2_df = pl.DataFrame(location_2)\n",
    "print(f\"Location Type: DataFrame 1: \", location_1_df)\n",
    "print(f\"Location Type: DataFrame 2: \", location_2_df)\n",
    "# type(location_1_df[\"location\"]) # will error out, because location_1 series didn't had column name\n",
    "type(location_2_df[\"location\"]), type(location_1), type(location_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame from a dict or a collection of dicts.\n",
    "# let's create a more sophisticated DataFrame\n",
    "# in real world, Organization maintain dozens of record structure to store \n",
    "# different type of locations, like ShipTo Location, Receiving, Mailing, Corp. office, head office,\n",
    "# field office etc. etc.\n",
    "\n",
    "## LOCATION DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "location = pl.DataFrame({\n",
    "    \"ID\":  list(range(11, 23)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\", \"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\", \"Dallas\",\"San Francisco\"],\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\"] * 3,\n",
    "    \"TYPE\" : \"Physical\",\n",
    "    \"CATEGORY\" : [\"Ship\",\"Recv\",\"Mfg\"] * 4\n",
    "})\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ACCOUNTS DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "accounts = pl.DataFrame({\n",
    "    \"ID\":  list(range(10000, 45000, 1000)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Operating Expenses\",\"Non Operating Expenses\",\"Assets\",\"Liabilities\",\"Net worth accounts\", \"Statistical Accounts\",\"Revenue\"] * 5,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\", \"Region E\"] * 7,\n",
    "    \"TYPE\" : [\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"] * 5,\n",
    "    \"CATEGORY\" : [\n",
    "       \t\t\"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n",
    "       \t\t\"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n",
    "       \t\t\"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n",
    "       \t\t\"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n",
    "       \t\t\"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n",
    "       \t\t\"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n",
    "       \t\t\"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"\n",
    "       \t],\n",
    "})\n",
    "accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (15, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>AS_OF_DATE</th><th>DESCRIPTION</th><th>REGION</th><th>STATUS</th><th>CLASSIFICATION</th><th>TYPE</th><th>CATEGORY</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>1000</td><td>2022-01-01 00:00:00</td><td>&quot;Sales &amp; Market…</td><td>&quot;Region A&quot;</td><td>&quot;Active&quot;</td><td>&quot;SALES&quot;</td><td>&quot;S&quot;</td><td>&quot;sales&quot;</td></tr><tr><td>1100</td><td>2022-01-01 00:00:00</td><td>&quot;Human Resource…</td><td>&quot;Region B&quot;</td><td>&quot;Active&quot;</td><td>&quot;HR&quot;</td><td>&quot;H&quot;</td><td>&quot;human_resource…</td></tr><tr><td>1200</td><td>2022-01-01 00:00:00</td><td>&quot;Information Te…</td><td>&quot;Region C&quot;</td><td>&quot;Active&quot;</td><td>&quot;IT&quot;</td><td>&quot;I&quot;</td><td>&quot;IT_Staff&quot;</td></tr><tr><td>1300</td><td>2022-01-01 00:00:00</td><td>&quot;Business leade…</td><td>&quot;Region A&quot;</td><td>&quot;Active&quot;</td><td>&quot;BUSINESS&quot;</td><td>&quot;B&quot;</td><td>&quot;business&quot;</td></tr><tr><td>1400</td><td>2022-01-01 00:00:00</td><td>&quot;other temp&quot;</td><td>&quot;Region B&quot;</td><td>&quot;Active&quot;</td><td>&quot;OTHERS&quot;</td><td>&quot;O&quot;</td><td>&quot;others&quot;</td></tr><tr><td>1500</td><td>2022-01-01 00:00:00</td><td>&quot;Sales &amp; Market…</td><td>&quot;Region C&quot;</td><td>&quot;Active&quot;</td><td>&quot;SALES&quot;</td><td>&quot;S&quot;</td><td>&quot;sales&quot;</td></tr><tr><td>1600</td><td>2022-01-01 00:00:00</td><td>&quot;Human Resource…</td><td>&quot;Region A&quot;</td><td>&quot;Active&quot;</td><td>&quot;HR&quot;</td><td>&quot;H&quot;</td><td>&quot;human_resource…</td></tr><tr><td>1700</td><td>2022-01-01 00:00:00</td><td>&quot;Information Te…</td><td>&quot;Region B&quot;</td><td>&quot;Active&quot;</td><td>&quot;IT&quot;</td><td>&quot;I&quot;</td><td>&quot;IT_Staff&quot;</td></tr><tr><td>1800</td><td>2022-01-01 00:00:00</td><td>&quot;Business leade…</td><td>&quot;Region C&quot;</td><td>&quot;Active&quot;</td><td>&quot;BUSINESS&quot;</td><td>&quot;B&quot;</td><td>&quot;business&quot;</td></tr><tr><td>1900</td><td>2022-01-01 00:00:00</td><td>&quot;other temp&quot;</td><td>&quot;Region A&quot;</td><td>&quot;Active&quot;</td><td>&quot;OTHERS&quot;</td><td>&quot;O&quot;</td><td>&quot;others&quot;</td></tr><tr><td>2000</td><td>2022-01-01 00:00:00</td><td>&quot;Sales &amp; Market…</td><td>&quot;Region B&quot;</td><td>&quot;Active&quot;</td><td>&quot;SALES&quot;</td><td>&quot;S&quot;</td><td>&quot;sales&quot;</td></tr><tr><td>2100</td><td>2022-01-01 00:00:00</td><td>&quot;Human Resource…</td><td>&quot;Region C&quot;</td><td>&quot;Active&quot;</td><td>&quot;HR&quot;</td><td>&quot;H&quot;</td><td>&quot;human_resource…</td></tr><tr><td>2200</td><td>2022-01-01 00:00:00</td><td>&quot;Information Te…</td><td>&quot;Region A&quot;</td><td>&quot;Active&quot;</td><td>&quot;IT&quot;</td><td>&quot;I&quot;</td><td>&quot;IT_Staff&quot;</td></tr><tr><td>2300</td><td>2022-01-01 00:00:00</td><td>&quot;Business leade…</td><td>&quot;Region B&quot;</td><td>&quot;Active&quot;</td><td>&quot;BUSINESS&quot;</td><td>&quot;B&quot;</td><td>&quot;business&quot;</td></tr><tr><td>2400</td><td>2022-01-01 00:00:00</td><td>&quot;other temp&quot;</td><td>&quot;Region C&quot;</td><td>&quot;Active&quot;</td><td>&quot;OTHERS&quot;</td><td>&quot;O&quot;</td><td>&quot;others&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (15, 8)\n",
       "┌──────┬────────────────┬───────────────┬──────────┬────────┬───────────────┬──────┬───────────────┐\n",
       "│ ID   ┆ AS_OF_DATE     ┆ DESCRIPTION   ┆ REGION   ┆ STATUS ┆ CLASSIFICATIO ┆ TYPE ┆ CATEGORY      │\n",
       "│ ---  ┆ ---            ┆ ---           ┆ ---      ┆ ---    ┆ N             ┆ ---  ┆ ---           │\n",
       "│ i64  ┆ datetime[μs]   ┆ str           ┆ str      ┆ str    ┆ ---           ┆ str  ┆ str           │\n",
       "│      ┆                ┆               ┆          ┆        ┆ str           ┆      ┆               │\n",
       "╞══════╪════════════════╪═══════════════╪══════════╪════════╪═══════════════╪══════╪═══════════════╡\n",
       "│ 1000 ┆ 2022-01-01     ┆ Sales &       ┆ Region A ┆ Active ┆ SALES         ┆ S    ┆ sales         │\n",
       "│      ┆ 00:00:00       ┆ Marketing     ┆          ┆        ┆               ┆      ┆               │\n",
       "│ 1100 ┆ 2022-01-01     ┆ Human         ┆ Region B ┆ Active ┆ HR            ┆ H    ┆ human_resourc │\n",
       "│      ┆ 00:00:00       ┆ Resource      ┆          ┆        ┆               ┆      ┆ e             │\n",
       "│ 1200 ┆ 2022-01-01     ┆ Information   ┆ Region C ┆ Active ┆ IT            ┆ I    ┆ IT_Staff      │\n",
       "│      ┆ 00:00:00       ┆ Technology    ┆          ┆        ┆               ┆      ┆               │\n",
       "│ 1300 ┆ 2022-01-01     ┆ Business      ┆ Region A ┆ Active ┆ BUSINESS      ┆ B    ┆ business      │\n",
       "│      ┆ 00:00:00       ┆ leaders       ┆          ┆        ┆               ┆      ┆               │\n",
       "│ …    ┆ …              ┆ …             ┆ …        ┆ …      ┆ …             ┆ …    ┆ …             │\n",
       "│ 2100 ┆ 2022-01-01     ┆ Human         ┆ Region C ┆ Active ┆ HR            ┆ H    ┆ human_resourc │\n",
       "│      ┆ 00:00:00       ┆ Resource      ┆          ┆        ┆               ┆      ┆ e             │\n",
       "│ 2200 ┆ 2022-01-01     ┆ Information   ┆ Region A ┆ Active ┆ IT            ┆ I    ┆ IT_Staff      │\n",
       "│      ┆ 00:00:00       ┆ Technology    ┆          ┆        ┆               ┆      ┆               │\n",
       "│ 2300 ┆ 2022-01-01     ┆ Business      ┆ Region B ┆ Active ┆ BUSINESS      ┆ B    ┆ business      │\n",
       "│      ┆ 00:00:00       ┆ leaders       ┆          ┆        ┆               ┆      ┆               │\n",
       "│ 2400 ┆ 2022-01-01     ┆ other temp    ┆ Region C ┆ Active ┆ OTHERS        ┆ O    ┆ others        │\n",
       "│      ┆ 00:00:00       ┆               ┆          ┆        ┆               ┆      ┆               │\n",
       "└──────┴────────────────┴───────────────┴──────────┴────────┴───────────────┴──────┴───────────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DEPARTMENT DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "dept = pl.DataFrame({\n",
    "    \"ID\":  list(range(1000, 2500, 100)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Sales & Marketing\",\"Human Resource\",\"Information Technology\",\"Business leaders\",\"other temp\"] * 3,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"] * 3,\n",
    "    \"TYPE\" : [\"S\",\"H\",\"I\",\"B\",\"O\"] * 3,\n",
    "    \"CATEGORY\" : [\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"] * 3,\n",
    "})\n",
    "dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LEDGER DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "sampleSize = 10\n",
    "org = \"ABC Inc.\"\n",
    "ledger = \"ACTUALS\" # BUDGET, STATS are other Ledger types\n",
    "fiscal_year_from = 2020\n",
    "fiscal_year_to = 2023\n",
    "random.seed(123)\n",
    "\n",
    "ledger = pl.DataFrame({\n",
    "\t\"LEDGER\" : ledger,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, fiscal_year_to+1, 1)),k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger = \"BUDGET\" # ACTUALS, STATS are other Ledger types\n",
    "\n",
    "ledger_budg = pl.DataFrame({\n",
    "\t\"LEDGER\" : ledger,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, fiscal_year_to+1, 1)),k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledger_budg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined ledger for Actuals and Budget\n",
    "dfLedger = pl.concat([ledger, ledger_budg], how=\"vertical\")\n",
    "dfLedger.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supply Chain DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRODUCT_CATEGORY DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "category = pl.DataFrame({\n",
    "    \"ID\":  list(range(1000, 2500, 100)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Rx\",\"Material\",\"Consulting\",\"Construction\",\"un-assigned\"] * 3,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"Rx\",\"Material\", \"Services\",\"Constructions\",\"OTHERS\"] * 3,\n",
    "    \"TYPE\" : [\"R\",\"M\",\"S\",\"C\",\"O\"] * 3,\n",
    "})\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRODUCT DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "product = pl.DataFrame({\n",
    "    \"ID\":  list(range(100, 250, 10)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Item 1\",\"Item 2\",\"Item 3\",\"Item 4\",\"Item 5\"] * 3,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CATEGORY\" : random.choices(category[\"ID\"], k=15),\n",
    "})\n",
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CUSTOMER DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "customer = pl.DataFrame({\n",
    "    \"ID\":  list(range(100, 250, 10)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Customer 1\",\"Customer 2\",\"Customer 3\",\"Customer 4\",\"Customer 5\"] * 3,\n",
    "    \"ADDRESS\" : [\"Address 1\",\"Address 2\",\"Address 3\",\"Address 4\",\"Address 5\"] * 3,\n",
    "    \"PHONE\" : [\"0000000001\",\"0000000002\",\"0000000003\",\"0000000004\",\"0000000005\"] * 3,\n",
    "    \"EMAIL\" : [\"1@email\",\"2@email\",\"3@email\",\"4@email\",\"5@email\"] * 3,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"TYPE\" : [\"Corp\",\"Gov\",\"Individual\"] * 5,\n",
    "    \"CATEGORY\" : random.choices(category[\"ID\"], k=15),\n",
    "})\n",
    "customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ORDER DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "sampleSize = 10\n",
    "\n",
    "order = pl.DataFrame({\n",
    "    \"ID\":  list(range(1000, 1100, sampleSize)),\n",
    "    \"AS_OF_DATE\" : datetime(2024, 1, 1),\n",
    "    \"CUSTOMER\": random.choices(customer[\"ID\"], k=sampleSize),\n",
    "    \"ITEM\": random.choices(product[\"ID\"], k=sampleSize),\n",
    "    \"QTY\": random.sample(range(1000000), sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INVOICE DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "sampleSize = 10\n",
    "\n",
    "invoice = pl.DataFrame({\n",
    "    \"AS_OF_DATE\" : datetime(2024, 1, 1),\n",
    "    \"ORDER\": random.choices(order[\"ID\"], k=sampleSize),\n",
    "    \"STATUS\" : [\"open\",\"paid\",\"cancelled\",\"shipped\",\"hold\"] * 2,\n",
    "})\n",
    "invoice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "---\n",
    "\n",
    "Polars is entirely based on Arrow data types and backed by Arrow memory arrays. This makes data processing cache-efficient and well-supported for Inter Process Communication.\n",
    "\n",
    "Please read official [Polars Data Type documentation](https://pola-rs.github.io/polars-book/user-guide/concepts/data-types/) for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### csv files ###\n",
    "#################\n",
    "dfLedger.write_csv(\"../downloads/ledger.csv\")\n",
    "dfLedger_c = pl.read_csv(\"../downloads/ledger.csv\")\n",
    "# Polars allows you to scan a CSV input. \n",
    "# Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a LazyFrame.\n",
    "dfLedger_c = pl.scan_csv(\"../downloads/ledger.csv\")\n",
    "\n",
    "#####################\n",
    "### parquet files ###\n",
    "#####################\n",
    "dfLedger.write_parquet(\"../downloads/ledger.parquet\")\n",
    "dfLedger_c = pl.read_parquet(\"../downloads/ledger.parquet\")\n",
    "# Polars allows you to scan a parquet input. \n",
    "# Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a LazyFrame.\n",
    "dfLedger_c = pl.scan_parquet(\"../downloads/ledger.parquet\")\n",
    "\n",
    "######################################################\n",
    "### json files ### ndjson: new line delimited json ###\n",
    "######################################################\n",
    "dfLedger.write_json(\"../downloads/ledger.json\")\n",
    "dfLedger_c = pl.scan_json(\"../downloads/ledger.json\")\n",
    "# Polars allows you to scan a json input. \n",
    "# Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a LazyFrame.\n",
    "dfLedger_c = pl.scan_json(\"../downloads/ledger.json\")\n",
    "\n",
    "####################\n",
    "## multiple files ##\n",
    "####################\n",
    "for i in range(5):\n",
    "    dfLedger.write_csv(f\"../downloads/my_many_files_{i}.csv\")\n",
    "pl.scan_csv(\"../downloads/my_many_files_*.csv\").show_graph() # see how query optimization/parallelism works\n",
    "df = pl.read_csv(\"../downloads/my_many_files_*.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "###############\n",
    "## databases ##\n",
    "###############\n",
    "import polars as pl\n",
    "\n",
    "connection_uri = \"postgres://username:password@server:port/database\"\n",
    "query = \"SELECT * FROM foo\"\n",
    "\n",
    "pl.read_database(query=query, connection_uri=connection_uri)\n",
    "\n",
    "# Polars doesn't manage connections and data transfer from databases by itself. \n",
    "# Instead external libraries (known as engines) handle this.\n",
    "#  At present Polars can use two engines to read from databases:\n",
    "# ConnectorX and ADBC\n",
    "# $  pip install connectorx\n",
    "# $  pip install adbc-driver-sqlite\n",
    "\n",
    "# As with reading from a database above Polars uses an engine to write to a database. \n",
    "# The currently supported engines are:\n",
    "# SQLAlchemy and\n",
    "# Arrow Database Connectivity (ADBC)\n",
    "# $  pip install SQLAlchemy pandas\n",
    "\n",
    "# AWS & Big Query - API - WIP as of 07/11/23"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars DataFrame Context\n",
    "---\n",
    "\n",
    "The two core components of the Polars DataFrame DSL (domain specific language) are Contexts and Expression.\n",
    "\n",
    "A context, as implied by the name, refers to the context in which an expression needs to be evaluated. There are three main contexts:\n",
    "\n",
    "    Selection: df.select([..]), df.with_columns([..])\n",
    "    Filtering: df.filter()\n",
    "    Groupby / Aggregation: df.groupby(..).agg([..])\n",
    "\n",
    "Additional Contexts\n",
    "    \n",
    "    List, Arrays and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select context\n",
    "out = df.select(\n",
    "    pl.sum(\"nrs\"),\n",
    "    pl.col(\"names\").sort(),\n",
    "    pl.col(\"names\").first().alias(\"first name\"),\n",
    "    (pl.mean(\"nrs\") * 10).alias(\"10xnrs\"),\n",
    ")\n",
    "print(out)\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.sum(\"nrs\").alias(\"nrs_sum\"),\n",
    "    pl.col(\"random\").count().alias(\"count\"),\n",
    ")\n",
    "print(df)\n",
    "\n",
    "## filter context\n",
    "out = df.filter(pl.col(\"nrs\") > 2)\n",
    "print(out)\n",
    "\n",
    "## group by context\n",
    "out = df.groupby(\"groups\").agg(\n",
    "    pl.sum(\"nrs\"),  # sum nrs by groups\n",
    "    pl.col(\"random\").count().alias(\"count\"),  # count group members\n",
    "    # sum random where name != null\n",
    "    pl.col(\"random\").filter(pl.col(\"names\").is_not_null()).sum().suffix(\"_sum\"),\n",
    "    pl.col(\"names\").reverse().alias((\"reversed names\")),\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars DataFrame Expressions\n",
    "---\n",
    "\n",
    "#### using Expression to select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection context\n",
    "# select context the selection applies expressions over columns.\n",
    "# The expressions in this context must produce Series that are all the same length or have a length of 1.\n",
    "\n",
    "# select all cols\n",
    "# select certain cols\n",
    "# select cols by data types\n",
    "# select cols including stats\n",
    "\n",
    "# context & operations\n",
    "# context & Data Type Casting, Struct Data Type, Strings\n",
    "# filter context\n",
    "# group by context\n",
    "# SQL Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### context & Data Type Casting, Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using Expression to apply conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using Expression to apply Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using Expression to handle missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using Expression to apply Structs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using Expression to apply List & Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using Expression to apply Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions, User defined functions and Windows function\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars Data Transformation\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars SQL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to use Lazy API\n",
    "\n",
    "---\n",
    "\n",
    "- the lazy API allows Polars to apply automatic query optimization with the query optimizer\n",
    "- the lazy API allows you to work with larger than memory datasets using streaming\n",
    "- the lazy API can catch schema errors before processing the data\n",
    "\n",
    "In the ideal case we use the lazy API right from a file as the query optimizer may help us to reduce the amount of data we read from the file.\n",
    "- scan_csv or scan_parquet or scan_xxx\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q1 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    ")\n",
    "```\n",
    "\n",
    "If we were to run the code above on the Reddit CSV the query would not be evaluated. \n",
    "Instead Polars takes each line of code, adds it to the internal query graph and optimizes the query graph.\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q4 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .collect()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution on larger-than-memory (RAM) data analytics\n",
    "\n",
    "If your data requires more memory than you have available Polars may be able to process the data in batches using streaming mode. To use streaming mode you simply pass the streaming=True argument to collect\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q5 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "```\n",
    "\n",
    "Execution on a partial dataset\n",
    "\n",
    "While you're writing, optimizing or checking your query on a large dataset, querying all available data may lead to a slow development process.\n",
    "\n",
    "You can instead execute the query with the .fetch method. The .fetch method takes a parameter n_rows and tries to 'fetch' that number of rows at the data source. The number of rows cannot be guaranteed, however, as the lazy API does not count how many rows there are at each stage of the query.\n",
    "\n",
    "Here we \"fetch\" 100 rows from the source file and apply the predicates.\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q9 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .fetch(n_rows=int(100))\n",
    ")\n",
    "```\n",
    "\n",
    "- TODO: cover streaming topic\n",
    "- TODO: cover sinking to a a file\n",
    "- TODO: all topics from Lazy API Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Query Optimization\n",
    "import polars as pl\n",
    "q3 = pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]}).lazy()\n",
    "\n",
    "print(q3.schema)\n",
    "\n",
    "q3.describe_optimized_plan()\n",
    "\n",
    "## query example to show schema\n",
    "lazy_eager_query = (\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"id\": [\"a\", \"b\", \"c\"],\n",
    "            \"month\": [\"jan\", \"feb\", \"mar\"],\n",
    "            \"values\": [0, 1, 2],\n",
    "        }\n",
    "    )\n",
    "    .lazy()\n",
    "    .with_columns((2 * pl.col(\"values\")).alias(\"double_values\"))\n",
    "    .collect()\n",
    "    .pivot(\n",
    "        index=\"id\", columns=\"month\", values=\"double_values\", aggregate_function=\"first\"\n",
    "    )\n",
    "    .lazy()\n",
    "    .filter(pl.col(\"mar\").is_null())\n",
    "    .collect()\n",
    ")\n",
    "print(lazy_eager_query)\n",
    "q3.show_graph(optimized=False)\n",
    "q3.explain(optimized=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RPA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
