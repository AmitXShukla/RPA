{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ultimate Guide to Python Polars DataFrame\n",
    "---\n",
    "\n",
    "    Author: Amit Shukla\n",
    "[https://github.com/AmitXShukla](https://github.com/AmitXShukla)\n",
    "\n",
    "The aim of this comprehensive user guide is to equip you with all the necessary knowledge and skills required to utilize Python Polars DataFrames effectively for financial and supply chain data science analytics. \n",
    "\n",
    "It provides an in-depth overview of the most commonly used functions and capabilities of the package.\n",
    "\n",
    "TODO: [download PDF version of this notebook](missinglink)\n",
    "\n",
    "## Table of content\n",
    "---\n",
    "\n",
    "- Why another DataFrame\n",
    "- Installation\n",
    "- Finance and Supply chain Data Analytics\n",
    "- Creating Polars DataFrame\n",
    "- Data Types\n",
    "- IO\n",
    "- Lazy API | Eager execution\n",
    "- About Contexts\n",
    "    - selection\n",
    "    - filtering\n",
    "    - group by\n",
    "- About Expressions\n",
    "- Transformation\n",
    "- Polar SQLs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why another DataFrame\n",
    "---\n",
    "Despite the numerous state-of-the-art dataframe packages available in the market, the Polar dataframe, which is built on RUST, boasts the fastest execution speed, enabling it to handle complex data science operations on tabular datasets.\n",
    "\n",
    "- Execution on larger-than-memory (RAM) data analytics\n",
    "- Lazy API vs Eager execution\n",
    "- Automatic Optimization\n",
    "- Embarrassingly Parallel\n",
    "- Easy to learn consistent, predictable API that has strict schema\n",
    "- SQLs like expressions\n",
    "\n",
    "#### Efficient Execution of Analytics on Large-than-Memory (RAM) Data\n",
    "\n",
    "RAM is not a big deal these days as most computers and VMs offer inexpensive GBs of RAM. In fact, the availability of affordable RAM is the primary reason why Pandas-like DataFrames remain the go-to choice, and it is unlikely that Pandas or R Tables will become obsolete anytime soon.\n",
    "\n",
    "However, Polars DataFrames are increasingly gaining popularity among developers due to their ability to harness the horsepower of Apache Spark, the backend support of DuckDB and Apache Arrow, and the ease-of-use of Pandas-like data frame functionalities. \n",
    "\n",
    "Additionally, Polars comes with built-in multi-core, multi-threaded parallel processing, making it a highly preferred choice.\n",
    "\n",
    "#### Lazy API vs Eager execution\n",
    "\n",
    "Just because an API is referred to as \"lazy\" does not necessarily imply that there will be a delay in processing or execution, and conversely, \"eager\" execution doesn't necessarily mean that the programming language will process data transformations or begin execution immediately and more quickly.\n",
    "\n",
    "In simpler terms, using a Lazy API implies that the API will first take the time to optimize the query before execution, which often results in improved performance.\n",
    "\n",
    "To illustrate this concept, consider running SQL on an RDBMS database. If the statistics, indexes, and data partitions have been appropriately optimized and the SQL is written in an optimized manner that utilizes the available statistics, indexes, and data partitions, the results will be delivered more quickly.\n",
    "\n",
    "#### Automatic Optimization\n",
    "\n",
    "We will learn few automation techniques to efficiently optimize queries.\n",
    "\n",
    "#### Embarrassingly Parallel\n",
    "\n",
    "#### Easy to learn consistent, predictable API that has strict schema\n",
    "\n",
    "#### SQLs like expressions\n",
    "\n",
    "`Let's get started`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U polars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finance and Supply chain Data Analytics\n",
    "---\n",
    "TODO:\n",
    "As stated above, since our objective is learn Data Science operations on Finance and Supply chain dataset, we will focus on creating\n",
    "few real life examples which are similar to Finance and Supply chain.\n",
    "\n",
    "For more information, please learn more about [Finance and Supply chain ERP data](https://amitxshukla.github.io/GeneralLedger.jl/tutorials/erd/).\n",
    "\n",
    "Objective of following section is to understand ERP GL like data. \n",
    "\n",
    "A sample of data structure and ERD relationship diagram can be seen in this diagram below.\n",
    "\n",
    "`Finance ER Diagram`\n",
    "\n",
    "![ERD Diagram](https://github.com/AmitXShukla/AmitXShukla.github.io/raw/master/blogs/PlutoCon/gl_erd.png)\n",
    "\n",
    "`Supply Chain ER Diagram`\n",
    "\n",
    "![ERD Diagram](../SampleData/ER_Flow.png)\n",
    "\n",
    "## Creating Polars DataFrame\n",
    "---\n",
    "\n",
    "#### Polars Data Structure\n",
    "\n",
    "The core base data structures provided by Polars are Series and DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "## Series and DataFrames\n",
    "#########################\n",
    "import polars as pl\n",
    "\n",
    "# with a tuple\n",
    "location_1 = pl.Series([\"CA\", \"OR\", \"WA\", \"TX\", \"NY\"]) \n",
    "# location_1 series when will converted to DataFrame will not have a column name\n",
    "# and hence, later when is used to create a dataframe will assign a column name like column_xx\n",
    "\n",
    "location_2 = pl.Series(\"location\", [\"CA\", \"OR\", \"WA\", \"TX\", \"NY\"])\n",
    "\n",
    "print(f\"Location Type: Series 1: \", location_1)\n",
    "print(f\"Location Type: Series 2: \", location_2)\n",
    "\n",
    "location_1_df = pl.DataFrame(location_1)\n",
    "location_2_df = pl.DataFrame(location_2)\n",
    "print(f\"Location Type: DataFrame 1: \", location_1_df)\n",
    "print(f\"Location Type: DataFrame 2: \", location_2_df)\n",
    "# type(location_1_df[\"location\"]) # will error out, because location_1 series didn't had column name\n",
    "type(location_2_df[\"location\"]), type(location_1), type(location_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame from a dict or a collection of dicts.\n",
    "# let's create a more sophisticated DataFrame\n",
    "# in real world, Organization maintain dozens of record structure to store \n",
    "# different type of locations, like ShipTo Location, Receiving, Mailing, Corp. office, head office,\n",
    "# field office etc. etc.\n",
    "\n",
    "## LOCATION DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "location = pl.DataFrame({\n",
    "    \"ID\":  list(range(11, 23)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\", \"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\", \"Dallas\",\"San Francisco\"],\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\"] * 3,\n",
    "    \"TYPE\" : \"Physical\",\n",
    "    \"CATEGORY\" : [\"Ship\",\"Recv\",\"Mfg\"] * 4\n",
    "})\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ACCOUNTS DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "accounts = pl.DataFrame({\n",
    "    \"ID\":  list(range(10000, 45000, 1000)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Operating Expenses\",\"Non Operating Expenses\",\"Assets\",\"Liabilities\",\"Net worth accounts\", \"Statistical Accounts\",\"Revenue\"] * 5,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\", \"Region E\"] * 7,\n",
    "    \"TYPE\" : [\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"] * 5,\n",
    "    \"CATEGORY\" : [\n",
    "       \t\t\"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n",
    "       \t\t\"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n",
    "       \t\t\"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n",
    "       \t\t\"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n",
    "       \t\t\"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n",
    "       \t\t\"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n",
    "       \t\t\"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"\n",
    "       \t],\n",
    "})\n",
    "accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPARTMENT DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "dept = pl.DataFrame({\n",
    "    \"ID\":  list(range(1000, 2500, 100)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Sales & Marketing\",\"Human Resource\",\"Information Technology\",\"Business leaders\",\"other temp\"] * 3,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"] * 3,\n",
    "    \"TYPE\" : [\"S\",\"H\",\"I\",\"B\",\"O\"] * 3,\n",
    "    \"CATEGORY\" : [\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"] * 3,\n",
    "})\n",
    "dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LEDGER DataFrame ##\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "sampleSize = 10\n",
    "org = \"ABC Inc.\"\n",
    "ledger = \"ACTUALS\" # BUDGET, STATS are other Ledger types\n",
    "fiscal_year_from = 2020\n",
    "fiscal_year_to = 2023\n",
    "random.seed(123)\n",
    "\n",
    "ledger = pl.DataFrame({\n",
    "\t\"LEDGER\" : ledger,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, fiscal_year_to+1, 1)),k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger = \"BUDGET\" # ACTUALS, STATS are other Ledger types\n",
    "\n",
    "ledger_budg = pl.DataFrame({\n",
    "\t\"LEDGER\" : ledger,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, fiscal_year_to+1, 1)),k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledger_budg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined ledger for Actuals and Budget\n",
    "dfLedger = pl.concat([ledger, ledger_budg], how=\"vertical\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "---\n",
    "\n",
    "Polars is entirely based on Arrow data types and backed by Arrow memory arrays. This makes data processing cache-efficient and well-supported for Inter Process Communication.\n",
    "\n",
    "Please read official [Polars Data Type documentation](https://pola-rs.github.io/polars-book/user-guide/concepts/data-types/) for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### csv files ###\n",
    "#################\n",
    "dfLedger.write_csv(\"../downloads/ledger.csv\")\n",
    "dfLedger_c = pl.read_csv(\"../downloads/ledger.csv\")\n",
    "# Polars allows you to scan a CSV input. \n",
    "# Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a LazyFrame.\n",
    "dfLedger_c = pl.scan_csv(\"../downloads/ledger.csv\")\n",
    "\n",
    "#####################\n",
    "### parquet files ###\n",
    "#####################\n",
    "dfLedger.write_parquet(\"../downloads/ledger.parquet\")\n",
    "dfLedger_c = pl.read_parquet(\"../downloads/ledger.parquet\")\n",
    "# Polars allows you to scan a parquet input. \n",
    "# Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a LazyFrame.\n",
    "dfLedger_c = pl.scan_parquet(\"../downloads/ledger.parquet\")\n",
    "\n",
    "######################################################\n",
    "### json files ### ndjson: new line delimited json ###\n",
    "######################################################\n",
    "dfLedger.write_json(\"../downloads/ledger.json\")\n",
    "dfLedger_c = pl.scan_json(\"../downloads/ledger.json\")\n",
    "# Polars allows you to scan a json input. \n",
    "# Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a LazyFrame.\n",
    "dfLedger_c = pl.scan_json(\"../downloads/ledger.json\")\n",
    "\n",
    "####################\n",
    "## multiple files ##\n",
    "####################\n",
    "for i in range(5):\n",
    "    dfLedger.write_csv(f\"../downloads/my_many_files_{i}.csv\")\n",
    "pl.scan_csv(\"../downloads/my_many_files_*.csv\").show_graph() # see how query optimization/parallelism works\n",
    "df = pl.read_csv(\"../downloads/my_many_files_*.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "###############\n",
    "## databases ##\n",
    "###############\n",
    "import polars as pl\n",
    "\n",
    "connection_uri = \"postgres://username:password@server:port/database\"\n",
    "query = \"SELECT * FROM foo\"\n",
    "\n",
    "pl.read_database(query=query, connection_uri=connection_uri)\n",
    "\n",
    "# Polars doesn't manage connections and data transfer from databases by itself. \n",
    "# Instead external libraries (known as engines) handle this.\n",
    "#  At present Polars can use two engines to read from databases:\n",
    "# ConnectorX and ADBC\n",
    "# $  pip install connectorx\n",
    "# $  pip install adbc-driver-sqlite\n",
    "\n",
    "# As with reading from a database above Polars uses an engine to write to a database. \n",
    "# The currently supported engines are:\n",
    "# SQLAlchemy and\n",
    "# Arrow Database Connectivity (ADBC)\n",
    "# $  pip install SQLAlchemy pandas\n",
    "\n",
    "# AWS & Big Query - API - WIP as of 07/11/23"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to use Lazy API\n",
    "\n",
    "---\n",
    "\n",
    "- the lazy API allows Polars to apply automatic query optimization with the query optimizer\n",
    "- the lazy API allows you to work with larger than memory datasets using streaming\n",
    "- the lazy API can catch schema errors before processing the data\n",
    "\n",
    "In the ideal case we use the lazy API right from a file as the query optimizer may help us to reduce the amount of data we read from the file.\n",
    "- scan_csv or scan_parquet or scan_xxx\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q1 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    ")\n",
    "```\n",
    "\n",
    "If we were to run the code above on the Reddit CSV the query would not be evaluated. \n",
    "Instead Polars takes each line of code, adds it to the internal query graph and optimizes the query graph.\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q4 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .collect()\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution on larger-than-memory (RAM) data analytics\n",
    "\n",
    "If your data requires more memory than you have available Polars may be able to process the data in batches using streaming mode. To use streaming mode you simply pass the streaming=True argument to collect\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q5 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "```\n",
    "\n",
    "Execution on a partial dataset\n",
    "\n",
    "While you're writing, optimizing or checking your query on a large dataset, querying all available data may lead to a slow development process.\n",
    "\n",
    "You can instead execute the query with the .fetch method. The .fetch method takes a parameter n_rows and tries to 'fetch' that number of rows at the data source. The number of rows cannot be guaranteed, however, as the lazy API does not count how many rows there are at each stage of the query.\n",
    "\n",
    "Here we \"fetch\" 100 rows from the source file and apply the predicates.\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q9 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .fetch(n_rows=int(100))\n",
    ")\n",
    "```\n",
    "\n",
    "- TODO: cover streaming topic\n",
    "- TODO: cover sinking to a a file\n",
    "- TODO: all topics from Lazy API Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Query Optimization\n",
    "import polars as pl\n",
    "q3 = pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]}).lazy()\n",
    "\n",
    "print(q3.schema)\n",
    "\n",
    "q3.describe_optimized_plan()\n",
    "\n",
    "## query example to show schema\n",
    "lazy_eager_query = (\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"id\": [\"a\", \"b\", \"c\"],\n",
    "            \"month\": [\"jan\", \"feb\", \"mar\"],\n",
    "            \"values\": [0, 1, 2],\n",
    "        }\n",
    "    )\n",
    "    .lazy()\n",
    "    .with_columns((2 * pl.col(\"values\")).alias(\"double_values\"))\n",
    "    .collect()\n",
    "    .pivot(\n",
    "        index=\"id\", columns=\"month\", values=\"double_values\", aggregate_function=\"first\"\n",
    "    )\n",
    "    .lazy()\n",
    "    .filter(pl.col(\"mar\").is_null())\n",
    "    .collect()\n",
    ")\n",
    "print(lazy_eager_query)\n",
    "q3.show_graph(optimized=False)\n",
    "q3.explain(optimized=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars DataFrame Context\n",
    "---\n",
    "\n",
    "A context, as implied by the name, refers to the context in which an expression needs to be evaluated. \n",
    "There are three main contexts:\n",
    "\n",
    "    Selection: df.select([..]), df.with_columns([..])\n",
    "    Filtering: df.filter()\n",
    "    Groupby / Aggregation: df.groupby(..).agg([..])\n",
    "\n",
    "The examples below are performed on the following DataFrame:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars DataFrame Expressions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars Data Transformation\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars SQL\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RPA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
