{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for Pro Finance Data Analysts\n",
    "\n",
    "A Comprehensive Guide to Exploratory Data Analysis with Real-Life Finance Statements such as SEC Filings.\n",
    "\n",
    "---\n",
    "TODO: Download PDF version of this notebook\n",
    "\n",
    "TODO: Video Tutorials\n",
    "\n",
    "    Author: Amit Shukla\n",
    "\n",
    "[https://github.com/AmitXShukla](https://github.com/AmitXShukla)\n",
    "\n",
    "[https://x.com/ashuklax](https://x.com/AShuklaX)\n",
    "\n",
    "[https://youtube.com/@Amit.Shukla](https://youtube.com/@Amit.Shukla)\n",
    "\n",
    "by the end of this blog, you will learn techniques to\n",
    "\n",
    "- Data Discovery using Pandas 2.0\n",
    "- Create Data ERD diagram with animation (using manim)\n",
    "- Data Visualization using Matplotlib\n",
    "- Data Visualization using PlotLy\n",
    "- Data Visualization using Seaborn\n",
    "- Analyze Distributions\n",
    "- Spot Anomalies\n",
    "- Test Hypothesis\n",
    "- Data Patterns\n",
    "- Check Assumptions\n",
    "- Create Interactive Visualizations\n",
    "- what-if Analysis\n",
    "- would, could, should\n",
    "- Time Travel on Time Series Data\n",
    "- Linear Regression, Auto Regression, SARIMA\n",
    "- SVM\n",
    "- Neural networks\n",
    "- Graph Computing\n",
    "\n",
    "---\n",
    "\n",
    "#### Introduction\n",
    "I'm Amit Shukla, and I specialize in training neural networks for finance supply chain analysis, enabling them to identify data patterns and make accurate predictions.\n",
    "During the challenges posed by the COVID-19 pandemic, I successfully trained GL and Supply Chain neural networks to anticipate supply chain shortages. The valuable insights gained from this effort have significantly influenced the content of this tutorial series.\n",
    "\t\n",
    "#### Objective:\n",
    "By delving into this powerful tool, we will master the fundamental techniques of using Exploratory Data Analysis. This knowledge is crucial in preparing finance and supply chain data for advanced analytics, visualization, and predictive modeling using neural networks and machine learning.\n",
    "\t\n",
    "#### Subject\n",
    "It's important to note that this particular series will concentrate solely on `Exploratory Data Analysis`.\n",
    "\t\n",
    "#### Following\n",
    "However, in future installments, we will explore Data Analytics and delve into the realm of machine learning for predictive analytics.\n",
    "\tThank you for joining me, and I'm excited to embark on this educational journey together.\n",
    "\t\n",
    "Let's get started.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "---\n",
    "\n",
    "- What is EDA\n",
    "- Installation\n",
    "- Technical & Fundamental analysis\n",
    "- Loading Finance, Supply chain and Stock prices data\n",
    "- Data Discovery using Pandas 2.0\n",
    "- Create Data ERD diagram with animation (using manim)\n",
    "- Data Visualization using Matplotlib\n",
    "- Data Visualization using PlotLy\n",
    "- Data Visualization using Seaborn\n",
    "- Analyze Distributions\n",
    "- Spot Anomalies\n",
    "- Test Hypothesis\n",
    "- Data Patterns\n",
    "- Check Assumptions\n",
    "- Create Interactive Visualizations\n",
    "- what-if Analysis\n",
    "- would, could, should\n",
    "- Time Travel on Time Series Data\n",
    "- Linear Regression, Auto Regression, SARIMA\n",
    "- SVM\n",
    "- Neural networks\n",
    "- Graph Computing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is EDA\n",
    "---\n",
    "EDA is often characterized as a tool for data analysts to discover patterns, spot anomalies, test a hypothesis, or check assumptions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install polars pandas numpy matplotlib seaborn tqdm\n",
    "\n",
    "# import platform;\n",
    "# print(platform.processor())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical & Fundamental Analysis\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is General Ledger\n",
    "\n",
    "GL serves as core of any Financial Management system.\n",
    "\n",
    "It's objective is to keep detail and summary accounting information and produce numerous financial reports for your organization.\n",
    "Typical, you will hear Cash Flow, Income and Balance Sheet statements as SEC filings as financial reports indicating Organizations financial growth.\n",
    "In general, accountants, statistical analysts strongly feel that financial reports from General Ledger are true indicator of organizations growth.\n",
    "\n",
    "## Technical Analysis\n",
    "\n",
    "is the process of forecasting future Organization growth or stock prices based on studying (using advance charting and applying mathematical formulas) past stock prices and trading volume.\n",
    "\n",
    "Technical analysis strongly believes that at any given point of time, stock price and trading volume reflects it current value and charting accurately captures all factors which can cause upwards or downwards stock prices movement.\n",
    "\n",
    "## Fundamental Analysis\n",
    "\n",
    "is the process of forecasting future Organization growth or stock prices based on studying company [Financial Statements](https://ir.tesla.com/#quarterly-disclosure) like Finance Ledger, Balance Sheet, Income, Cash Flow Statements.\n",
    "\n",
    "## Techno-Fundamental Analysis\n",
    "\n",
    "In this notebook, I am proposing to use 3rd type of analysis.\n",
    "With the use of Machine Learning, One can apply Statistical Analysis, ML algorithms to apply statistical data associations to Ledger, Sub-Ledger (accounting entries) statements along with Company Stock prices and trading volume.\n",
    "\n",
    "`Techno-fundamental analysis is not new, however, its seen very difficult because its requires big data and large and fast computations for large data sets and great assets for Statistical programming.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finance data model\n",
    "\n",
    "A finance data model is a comprehensive and structured framework used to represent and organize financial information within an organization.\n",
    "\n",
    "It serves as the blueprint for how financial data is collected, stored, processed, and analyzed, ensuring accuracy, consistency, and efficiency in managing financial operations. \n",
    "\n",
    "The model defines the relationships between various financial entities such as assets, liabilities, revenues, expenses, and equity, enabling financial professionals to gain insights into the company's financial health, performance, and risk exposure. \n",
    "\n",
    "It typically encompasses multiple dimensions, including time, currency, and geographical locations, chart of accounts, departments / cost centers, fiscal years and reporting accounting periods providing a holistic view of the organization's financial landscape. \n",
    "\n",
    "A well-designed finance data model is critical for generating accurate financial reports, facilitating financial planning and forecasting, and supporting strategic decision-making at all levels of the business.\n",
    "\n",
    "As stated above, since our objective is learn Data Science operations on Finance and Supply chain dataset, we will focus on creating\n",
    "few real life examples which are similar to Finance and Supply chain.\n",
    "\n",
    "For more information, please learn more about [Finance and Supply chain ERP data](https://amitxshukla.github.io/GeneralLedger.jl/tutorials/erd/).\n",
    "\n",
    "Objective of following section is to understand ERP GL like data. \n",
    "\n",
    "A sample of data structure and ERD relationship diagram can be seen in this diagram below.\n",
    "\n",
    "The ERD presented below depicts the data set that serves as the foundation for generating Finance Statements.\n",
    "\n",
    "`Finance ER Diagram`\n",
    "\n",
    "![ERD Diagram](https://github.com/AmitXShukla/AmitXShukla.github.io/raw/master/blogs/PlutoCon/gl_erd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supply chain data model\n",
    "\n",
    "A supply chain data model is a structured representation of the various elements and interactions within a supply chain network.\n",
    "\n",
    "It encompasses critical components such as customers, orders, receipts, products, invoices, vouchers, and ship-to locations. \n",
    "\n",
    "Customers form the foundation of the supply chain, as they drive demand for products. Orders and receipts represent the flow of goods and services, capturing the movement of inventory throughout the supply chain. \n",
    "\n",
    "The product entity accounts for the diverse range of items being handled, from raw materials to finished goods.\n",
    "\n",
    "Invoices and vouchers track financial transactions, ensuring transparent and accurate billing processes. \n",
    "\n",
    "Ship-to locations specify the destinations of goods during the distribution process.\n",
    "\n",
    "By establishing relationships and attributes between these elements, the supply chain data model aids in optimizing inventory management, forecasting demand, enhancing order fulfillment, and ultimately, improving overall operational efficiency within the supply chain ecosystem.\n",
    "\n",
    "`Supply Chain ER Diagram`\n",
    "\n",
    "![ERD Diagram](../SampleData/ER_Flow.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Finance, Supply chain and Stock Prices Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial & stock prices data\n",
    "as stated in earlier sections, we will use real life examples (Tesla Inc.) in our analysis.\n",
    "We will first download real life Finance statement data and then later we will derive/create synthetic data from These Finance statements for our exploratory data analysis purpose.\n",
    "\n",
    "download Financial Statement data to support Fundamental analysis\n",
    "[https://ir.tesla.com/sec-filings](https://ir.tesla.com/sec-filings)\n",
    "\n",
    "download Stock market data to support Technical analysis\n",
    "[https://finance.yahoo.com/quote/TSLA/history/](https://finance.yahoo.com/quote/TSLA/history/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amit_TestQRcode.png',\n",
       " 'ER_Flow.png',\n",
       " 'Process_Flow.png',\n",
       " 'sampleData.csv',\n",
       " 'The Ultimate Guide to Data Wrangling with Python - Rust Polars Data Frame.pdf',\n",
       " 'TSLA.csv',\n",
       " 'TSLA_Fin_Statements.xlsx']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, shutil\n",
    "os.listdir(\"../SampleData\")\n",
    "\n",
    "# we will work through TSLA**.csv files in below sections\n",
    "# you will see that these files contain real data\n",
    "# and real data is very messy in real world,\n",
    "# so we will spend much time in data cleansing and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load SEC Filings and Other Finance market data\n",
    "\n",
    "please note that we have extensively discussed this topic in past tutorials. Below are couple of links to video tutorials that can assist you in extracting information from a specific page, like a website contraining links to download SEC filings and Financial statemetns. You can utilize the following links to download data and streamline the downloading process.\n",
    "\n",
    "Please be aware that web scrapping may not be advisable due to the sensitive nature of Finance Statements. Ensure you obtain appropriate approvals and rely on authentic APIs when gathering such data.\n",
    "\n",
    "Download csv, pdf, xls data files from web pages using [Open AI ChatGPT Python code](https://youtu.be/-vGuHLx0DEw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is used to download Finance Statements data from Edgar or SEC filing website\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm # show a progress meter, wrap any iterable in tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# define URLs\n",
    "download_URL = \"https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/\"\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "\n",
    "period = 2015\n",
    "qtr = 1\n",
    "url = f\"{download_URL}{period}q{qtr}_notes.zip\"\n",
    "target = Path(\"../downloads/SEC\")\n",
    "response = requests.get(url, headers={\"User-Agent\": user_agent}).content\n",
    "with ZipFile(BytesIO(response)) as zip_file:\n",
    "    for file in zip_file.namelist():\n",
    "        local_file = target\n",
    "        if local_file.exists():\n",
    "            continue\n",
    "        with local_file.open(\"wb\") as output:\n",
    "                for line in zip_file.open(file).readlines():\n",
    "                    output.write(line)\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../downloads\")) # show downloaded file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Discovery using Pandas 2.0\n",
    "---\n",
    "In the previous section, we downloaded data locally.\n",
    "\n",
    "In this section, we will learn techniques for reading, transformation and understanding data using Pandas framework.\n",
    "\n",
    "Please [click here](https://www.youtube.com/playlist?list=PLp0TENYyY8lHJaY4t5bAihnFS5TBUQYV1) if you are interested to learn more about using `Rust Polars DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install polars pandas numpy matplotlib seaborn openpyxl\n",
    "# I assume, you have already created an EDA virtual environment and installed these packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Exporting data\n",
    "The Pandas library offers functionalities for importing and exporting data in various formats. The syntax for most of these methods, such as `read_csv`, `read_excel`, or `read_parquet`, is quiet similar. The same applies to writing data with methods like `to_csv`, `to_excel` and `to_parquet`.\n",
    "\n",
    "There's no need to memorize all these methods as there are numerous ones, each with different options and parameters. It;s more beneficial to understand the method signatures and experiment with the options while working with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['balancesheet', 'income', 'cashflow'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.listdir(\"../SampleData/\")\n",
    "df1 = pd.read_csv(\"../SampleData/TSLA.csv\")\n",
    "df1.sample(5)\n",
    "\n",
    "df_temp = pd.ExcelFile(\"../SampleData/TSLA_Fin_Statements.xlsx\")\n",
    "df_temp.sheet_names\n",
    "# df21 = pd.read_excel(\"../SampleData/TSLA_Fin_Statements.xlsx\", sheet_name=\"balancesheet\")\n",
    "# df21.describe()\n",
    "# df21.head(5)\n",
    "\n",
    "# df22 = pd.read_excel(\"../SampleData/TSLA_Fin_Statements.xlsx\", sheet_name=\"income\")\n",
    "# df22.describe()\n",
    "# df22.head(5)\n",
    "\n",
    "df23 = pd.read_excel(\"../SampleData/TSLA_Fin_Statements.xlsx\", sheet_name=\"cashflow\")\n",
    "df23.describe()\n",
    "df23.tail(5)\n",
    "\n",
    "# alternatively, use ths syntex to read all excel sheets into data frame\n",
    "# # Each Excel sheet in a Python dictionary\n",
    "# workbook = pd.ExcelFile('../SampleData/TSLA_Fin_Statements.xlsx')\n",
    "# dictionary = {}\n",
    "# for sheet_name in workbook.sheet_names:\n",
    "#     df = workbook.parse(sheet_name)\n",
    "#     dictionary[sheet_name] = df\n",
    "# Note: the parse() method takes many arguments\n",
    "# dictionary.keys()\n",
    "    \n",
    "###########################\n",
    "### write results to csv\n",
    "###########################\n",
    "# to_csv is used to write dataframes into csv\n",
    "# df1.to_csv(\"../SampleData/TSLA_downloaded.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "The core base data structure provided by Pandas is Series and DataFrame.\n",
    "\n",
    "**`Series`** is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the `index`. \n",
    "\n",
    "**`DataFrame`** is a two-dimensional labeled array capable of holding `Series`(s)/columns or sometime referred as `vectors` of any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the `index`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.653677\n",
      "1   -1.337561\n",
      "2   -0.717682\n",
      "3   -0.452272\n",
      "4   -0.057651\n",
      "dtype: float64\n",
      "RangeIndex(start=0, stop=5, step=1)\n",
      "   one  two\n",
      "a  1.0  1.0\n",
      "b  2.0  2.0\n",
      "c  3.0  3.0\n",
      "d  NaN  4.0\n",
      "Index(['a', 'b', 'c', 'd'], dtype='object')\n",
      "RangeIndex(start=0, stop=5, step=1)\n",
      "Index(['one', 'two'], dtype='object')\n",
      "Index(['a', 'b', 'c', 'd'], dtype='object')\n",
      "axes:\n",
      "[Index(['a', 'b', 'c', 'd'], dtype='object'), Index(['one', 'two'], dtype='object')]\n",
      "False\n",
      "True\n",
      "False\n",
      "1\n",
      "['a' 'b' 'c' 'd']\n",
      "<bound method IndexOpsMixin.value_counts of Index(['a', 'b', 'c', 'd'], dtype='object')>\n",
      "['a', 'b', 'c', 'd']\n",
      "4\n",
      "a\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "s = pd.Series(np.random.randn(5))\n",
    "# s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "\n",
    "print(s)\n",
    "print(s.index)\n",
    "\n",
    "# similarly a DataFrame is made up of series/vectors as columns\n",
    "d = {\n",
    "    \"one\": pd.Series([1.0, 2.0, 3.0], index=[\"a\", \"b\", \"c\"]),\n",
    "    \"two\": pd.Series([1.0, 2.0, 3.0, 4.0], index=[\"a\", \"b\", \"c\", \"d\"]),\n",
    "}\n",
    "df = pd.DataFrame(d)\n",
    "print(df)\n",
    "print(df.index)\n",
    "\n",
    "# --- get Index from Series and DataFrame\n",
    "print(s.index)\n",
    "print(df.columns)\n",
    "print(df.index)\n",
    "print(\"axes:\")\n",
    "print(df.axes)\n",
    "\n",
    "# --- Index properties\n",
    "print(df.index.is_monotonic_decreasing)\n",
    "print(df.index.is_monotonic_increasing)\n",
    "print(df.index.has_duplicates)\n",
    "print(df.index.nlevels)\n",
    "\n",
    "# --- Index methods\n",
    "print(df.index.values)\n",
    "print(df.index.value_counts)\n",
    "print(df.index.tolist())\n",
    "print(df.index.nunique())\n",
    "print(df.index.min())\n",
    "print(df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            Boston\n",
      "1          New York\n",
      "2      Philadelphia\n",
      "3         Cleveland\n",
      "4          Richmond\n",
      "5           Atlanta\n",
      "6           Chicago\n",
      "7         St. Louis\n",
      "8       Minneapolis\n",
      "9       Kansas City\n",
      "10           Dallas\n",
      "11    San Francisco\n",
      "Name: DESCRIPTION, dtype: object\n",
      "(12, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>AS_OF_DATE</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>REGION</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Region A</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>New York</td>\n",
       "      <td>Region B</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Recv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Region C</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Mfg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>Region D</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Region A</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Recv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID AS_OF_DATE   DESCRIPTION    REGION      TYPE CATEGORY\n",
       "0  11 2022-01-01        Boston  Region A  Physical     Ship\n",
       "1  12 2022-01-01      New York  Region B  Physical     Recv\n",
       "2  13 2022-01-01  Philadelphia  Region C  Physical      Mfg\n",
       "3  14 2022-01-01     Cleveland  Region D  Physical     Ship\n",
       "4  15 2022-01-01      Richmond  Region A  Physical     Recv"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example series and Data frame\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "descr = pd.Series([\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n",
    "                     \"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n",
    "                     \"Dallas\",\"San Francisco\"], name = \"DESCRIPTION\")\n",
    "print(descr)\n",
    "\n",
    "location = pd.DataFrame({\n",
    "    \"ID\":  list(range(11, 23)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : descr,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\"] * 3,\n",
    "    \"TYPE\" : \"Physical\",\n",
    "    \"CATEGORY\" : [\"Ship\",\"Recv\",\"Mfg\"] * 4\n",
    "})\n",
    "print(location.shape)\n",
    "location.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accounts.csv', 'dept.csv', 'earth.jpg', 'ledger.csv', 'ledger.json', 'ledger.parquet', 'location.csv']\n",
      "['accounts.csv', 'dept.csv', 'earth.jpg', 'ledger.csv', 'ledger.json', 'ledger.parquet', 'location.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200000, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this script to create synthetic Finance, Supply chain dataset\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dirPath = \"../../../downloads/\" # directory where sample csv are generated\n",
    "sampleSize = 100_000 # generate 100k sample rows\n",
    "\n",
    "print(os.listdir(dirPath))\n",
    "\n",
    "# Creating DataFrame from a dict or a collection of dicts.\n",
    "# let's create a more sophisticated DataFrame\n",
    "# in real world, Organization maintain dozens of record structure to store \n",
    "# different type of locations, like ShipTo Location, Receiving, \n",
    "# Mailing, Corp. office, head office,\n",
    "# field office etc. etc.\n",
    "\n",
    "########################\n",
    "## LOCATION DataFrame ##\n",
    "########################\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "location = pd.DataFrame({\n",
    "    \"ID\":  list(range(11, 23)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n",
    "                     \"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n",
    "                     \"Dallas\",\"San Francisco\"],\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\"] * 3,\n",
    "    \"TYPE\" : \"Physical\",\n",
    "    \"CATEGORY\" : [\"Ship\",\"Recv\",\"Mfg\"] * 4\n",
    "})\n",
    "location.head()\n",
    "\n",
    "########################\n",
    "## ACCOUNTS DataFrame ##\n",
    "########################\n",
    "\n",
    "accounts = pd.DataFrame({\n",
    "    \"ID\":  list(range(10000, 45000, 1000)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Operating Expenses\",\"Non Operating Expenses\",\"Assets\",\n",
    "                     \"Liabilities\",\"Net worth accounts\", \"Statistical Accounts\",\n",
    "                     \"Revenue\"] * 5,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\", \"Region E\"] * 7,\n",
    "    \"TYPE\" : [\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \n",
    "                        \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\n",
    "                        \"REVENUE\"] * 5,\n",
    "    \"CATEGORY\" : [\n",
    "       \t\t\"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n",
    "       \t\t\"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n",
    "       \t\t\"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n",
    "       \t\t\"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n",
    "       \t\t\"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n",
    "       \t\t\"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n",
    "       \t\t\"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"\n",
    "       \t],\n",
    "})\n",
    "accounts.head()\n",
    "\n",
    "##########################\n",
    "## DEPARTMENT DataFrame ##\n",
    "##########################\n",
    "\n",
    "dept = pd.DataFrame({\n",
    "    \"ID\":  list(range(1000, 2500, 100)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Sales & Marketing\",\"Human Resource\",\n",
    "                     \"Information Technology\",\"Business leaders\",\"other temp\"] * 3,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"] * 3,\n",
    "    \"TYPE\" : [\"S\",\"H\",\"I\",\"B\",\"O\"] * 3,\n",
    "    \"CATEGORY\" : [\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"] * 3,\n",
    "})\n",
    "dept.head()\n",
    "\n",
    "######################\n",
    "## LEDGER DataFrame ##\n",
    "######################\n",
    "\n",
    "org = \"ABC Inc.\"\n",
    "ledger_type = \"ACTUALS\" # BUDGET, STATS are other Ledger types\n",
    "fiscal_year_from = 2020\n",
    "fiscal_year_to = 2023\n",
    "random.seed(123)\n",
    "\n",
    "ledger = pd.DataFrame({\n",
    "\t\"LEDGER\" : ledger_type,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, \n",
    "                                          fiscal_year_to+1, 1)),k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledger.sample(5)\n",
    "\n",
    "ledger_type = \"BUDGET\" # ACTUALS, STATS are other Ledger types\n",
    "\n",
    "ledgerBudget = pd.DataFrame({\n",
    "\t\"LEDGER\" : ledger_type,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, fiscal_year_to+1, 1))\n",
    "                               ,k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledgerBudget.sample(5)\n",
    "\n",
    "#########################################\n",
    "# combined ledger for Actuals and Budget\n",
    "#########################################\n",
    "dfLedger = pd.concat([ledger, ledgerBudget])\n",
    "dfLedger.sample(5)\n",
    "\n",
    "location.to_csv(f\"{dirPath}location.csv\")\n",
    "dept.to_csv(f\"{dirPath}dept.csv\")\n",
    "accounts.to_csv(f\"{dirPath}accounts.csv\")\n",
    "dfLedger.to_csv(f\"{dirPath}ledger.csv\")\n",
    "\n",
    "print(os.listdir(dirPath))\n",
    "dfLedger.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration using Pandas basics functionalities & Indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 200000 entries, 0 to 99999\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   LEDGER        200000 non-null  object\n",
      " 1   ORG           200000 non-null  object\n",
      " 2   FISCAL_YEAR   200000 non-null  int64 \n",
      " 3   PERIOD        200000 non-null  int64 \n",
      " 4   ACCOUNT       200000 non-null  int64 \n",
      " 5   DEPT          200000 non-null  int64 \n",
      " 6   LOCATION      200000 non-null  int64 \n",
      " 7   POSTED_TOTAL  200000 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 13.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now, we will start with simple example\n",
    "# as you can see, TSLA.csv which has historical stock prices and is easier to read\n",
    "# once we lean basics of indexing/viewing functionalities, we will use these techniques later\n",
    "# to standardize more complex data as shown in TSLA Financial Statement file\n",
    "\n",
    "##########################\n",
    "## quick glance through ##\n",
    "##########################\n",
    "dfLedger.head(3)\n",
    "dfLedger.tail(3)\n",
    "dfLedger.sample\n",
    "dfLedger.ndim\n",
    "dfLedger.axes\n",
    "dfLedger.size\n",
    "dfLedger.shape\n",
    "dfLedger.index\n",
    "dfLedger.index.array\n",
    "dfLedger.columns\n",
    "dfLedger.dtypes\n",
    "dfLedger.values\n",
    "\n",
    "# # DataFrame iteration methods\n",
    "# dfLedger.iteritems()# (col-index, Series) pairs # NA - will fail\n",
    "dfLedger.iterrows() # (row-index, Series) pairs\n",
    "\n",
    "dfLedger[\"LEDGER\"].value_counts() # The value_counts() Series method computes a histogram of a 1D array of values. It can also be used as a function on regular arrays:\n",
    "data = {\"a\": [1, 2, 3, 4], \"b\": [\"x\", \"x\", \"y\", \"y\"]}\n",
    "frame = pd.DataFrame(data)\n",
    "frame.value_counts()\n",
    "\n",
    "df1 = pd.read_csv(\"../SampleData/TSLA.csv\")\n",
    "df1[\"Date\"].__len__()\n",
    "df1[\"Date\"].values\n",
    "df1[\"Date\"].to_numpy(dtype=object)\n",
    "df1[\"Date\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "df1.to_numpy()\n",
    "df1[\"Close\"].array\n",
    "df1.describe()\n",
    "df1[\"Close\"].describe()\n",
    "\n",
    "####################################\n",
    "## data Descriptive statistics #####\n",
    "## count, sum, mean, median, min, \n",
    "####################################\n",
    "dfLedger.info()\n",
    "dfLedger.describe()\n",
    "df1[\"High\"].count()\n",
    "# count # Number of non-NA observations\n",
    "# sum # Sum of values\n",
    "# mean # Mean of values\n",
    "# median # Arithmetic median of values\n",
    "# min # Minimum\n",
    "# max # Maximum\n",
    "# idxmin() and idxmax() \n",
    "# mode # Mode\n",
    "# abs # Absolute Value\n",
    "# prod # Product of values\n",
    "# std # Bessel-corrected sample standard deviation\n",
    "# var # Unbiased variance\n",
    "# sem # Standard error of the mean\n",
    "# skew # Sample skewness (3rd moment)\n",
    "# kurt # Sample kurtosis (4th moment)\n",
    "# quantile # Sample quantile (value at %)\n",
    "# cumsum # Cumulative sum\n",
    "# cumprod # Cumulative product\n",
    "# cummax # Cumulative maximum\n",
    "# cummin # Cumulative minimum\n",
    "\n",
    "# df1[\"High\"].cummin()\n",
    "\n",
    "# element-wise methods\n",
    "# df1['High'].isnull()\n",
    "# df1['High'].notnull() # not isnull()\n",
    "# df1['High'].astype(float)\n",
    "# df1['High'].round(decimals=0)\n",
    "# df1['High'].diff(periods=1)\n",
    "# df1['High'].shift(periods=1)\n",
    "# df1['Date'].to_datetime()\n",
    "# df1['High'].fillna(0) # replace NaN w 0\n",
    "# df1['High'].cumsum()\n",
    "# df1['High'].cumprod()\n",
    "# df1['High'].pct_change(periods=4)\n",
    "# df1['High'].rolling_sum(periods=4, window=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LEDGER', 'ORG', 'FISCAL_YEAR', 'PERIOD', 'ACCOUNT', 'DEPT', 'LOCATION',\n",
      "       'POSTED_TOTAL'],\n",
      "      dtype='object')\n",
      "['LEDGER', 'ORG', 'FISCAL_YEAR', 'PERIOD', 'ACCOUNT', 'DEPT', 'LOCATION', 'POSTED_TOTAL']\n",
      "['LEDGER', 'ORGANIZATION', 'FY', 'PERIOD', 'ACCOUNT', 'DEPT', 'LOCATION', 'POSTED_TOTAL']\n",
      "['LEDGER', 'ORG', 'FISCAL_YEAR', 'PERIOD', 'ACCOUNT', 'DEPT', 'LOCATION', 'POSTED_TOTAL']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEDGER</th>\n",
       "      <th>ORG</th>\n",
       "      <th>FISCAL_YEAR</th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>ACCOUNT</th>\n",
       "      <th>DEPT</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>POSTED_TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>12000</td>\n",
       "      <td>2400</td>\n",
       "      <td>22</td>\n",
       "      <td>753956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>1900</td>\n",
       "      <td>14</td>\n",
       "      <td>826906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>21000</td>\n",
       "      <td>1700</td>\n",
       "      <td>17</td>\n",
       "      <td>454574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>34000</td>\n",
       "      <td>1300</td>\n",
       "      <td>12</td>\n",
       "      <td>334989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>14000</td>\n",
       "      <td>2100</td>\n",
       "      <td>11</td>\n",
       "      <td>290813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>16000</td>\n",
       "      <td>1900</td>\n",
       "      <td>14</td>\n",
       "      <td>587255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>26000</td>\n",
       "      <td>1000</td>\n",
       "      <td>17</td>\n",
       "      <td>663831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>20000</td>\n",
       "      <td>2400</td>\n",
       "      <td>17</td>\n",
       "      <td>65437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>11000</td>\n",
       "      <td>1900</td>\n",
       "      <td>17</td>\n",
       "      <td>960253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>42000</td>\n",
       "      <td>2300</td>\n",
       "      <td>11</td>\n",
       "      <td>41157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LEDGER       ORG  FISCAL_YEAR  PERIOD  ACCOUNT  DEPT  LOCATION  \\\n",
       "0      ACTUALS  ABC Inc.         2020      10    12000  2400        22   \n",
       "1      ACTUALS  ABC Inc.         2020       1    10000  1900        14   \n",
       "2      ACTUALS  ABC Inc.         2021      12    21000  1700        17   \n",
       "3      ACTUALS  ABC Inc.         2020       3    34000  1300        12   \n",
       "4      ACTUALS  ABC Inc.         2023       9    14000  2100        11   \n",
       "...        ...       ...          ...     ...      ...   ...       ...   \n",
       "99995   BUDGET  ABC Inc.         2023       7    16000  1900        14   \n",
       "99996   BUDGET  ABC Inc.         2023       1    26000  1000        17   \n",
       "99997   BUDGET  ABC Inc.         2022       5    20000  2400        17   \n",
       "99998   BUDGET  ABC Inc.         2020       8    11000  1900        17   \n",
       "99999   BUDGET  ABC Inc.         2022       7    42000  2300        11   \n",
       "\n",
       "       POSTED_TOTAL  \n",
       "0            753956  \n",
       "1            826906  \n",
       "2            454574  \n",
       "3            334989  \n",
       "4            290813  \n",
       "...             ...  \n",
       "99995        587255  \n",
       "99996        663831  \n",
       "99997         65437  \n",
       "99998        960253  \n",
       "99999         41157  \n",
       "\n",
       "[200000 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "## working with columns ##\n",
    "##########################\n",
    "\n",
    "print(dfLedger.columns)\n",
    "print(dfLedger.columns.tolist())\n",
    "dfLedger.rename(columns={'FISCAL_YEAR':'FY', 'ORG':'ORGANIZATION'}, inplace=True)\n",
    "print(dfLedger.columns.tolist())\n",
    "dfLedger.rename(columns={'FY':'FISCAL_YEAR','ORGANIZATION':'ORG'}, inplace=True)\n",
    "print(dfLedger.columns.tolist())\n",
    "\n",
    "# Selecting columns\n",
    "dfLedger['LEDGER'] # returns a series datatype\n",
    "dfLedger[['LEDGER']] # returns a data frame datatype\n",
    "dfLedger[['LEDGER', 'FISCAL_YEAR']] # returns a data frame datatype\n",
    "\n",
    "dfLedger[dfLedger.columns[0]] # select column by position\n",
    "dfLedger[dfLedger.columns[[0,1,2]]] # select columns by position\n",
    "\n",
    "dfLedger[\"FISCAL_YEAR\"]\n",
    "dfLedger.FISCAL_YEAR\n",
    "dfLedger[\"FISCAL_YEAR\"].value_counts()\n",
    "# add a new column\n",
    "dfLedger[\"FYP\"] = dfLedger[\"FISCAL_YEAR\"].astype(str) + \"-\" + dfLedger[\"PERIOD\"].astype(str)\n",
    "dfLedger.FYP\n",
    "# dfLedger.pop(\"FYP\")\n",
    "dfLedger.drop(\"FYP\", axis=1, inplace=True)\n",
    "# del dfLedger['FYP']\n",
    "\n",
    "# Vectorised column calculations \n",
    "# this is very useful in feature normalization | standardization\n",
    "dfLedger['ranked']=dfLedger['POSTED_TOTAL']*100000/sum(dfLedger.POSTED_TOTAL)\n",
    "max(dfLedger.ranked)\n",
    "min(dfLedger.ranked)\n",
    "\n",
    "# other numpy mathematical functions to columns\n",
    "import numpy as np\n",
    "np.seterr(divide = 'ignore') # ignore log func divide by zero warning\n",
    "dfLedger['new_ranked'] = np.log(dfLedger['POSTED_TOTAL'])\n",
    "dfLedger['new_ranked'] = np.round(dfLedger['new_ranked'],2)\n",
    "\n",
    "del dfLedger[\"ranked\"]\n",
    "del dfLedger[\"new_ranked\"]\n",
    "\n",
    "# Columns value set based on criteria\n",
    "dfLedger['POSTED_TOTAL']=dfLedger['POSTED_TOTAL'].where(dfLedger['POSTED_TOTAL']>0,other=0)\n",
    "dfLedger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290813"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting columns with .loc, .iloc and .ix\n",
    "dfLedger.loc[:, 'LEDGER':'PERIOD'] # inclusive\n",
    "dfLedger.iloc[:, 0:4] # exclusive\n",
    "# Get the integer position of a column index label\n",
    "dfLedger.columns.get_loc('POSTED_TOTAL')\n",
    "\n",
    "# Selecting scalars with .at, .iat\n",
    "dfLedger.at[4, 'POSTED_TOTAL'] # inclusive\n",
    "dfLedger.iat[4, 7] # exclusive\n",
    "\n",
    "# filter selections\n",
    "dfLedger.filter(items=['ORG']) # by col\n",
    "dfLedger.filter(like='D') # keep D in col\n",
    "dfLedger.filter(regex='D') # regex in col\n",
    "\n",
    "###############################\n",
    "## working with rows & cells ##\n",
    "###############################\n",
    "dfLedger.index.to_list()\n",
    "# dfLedger.rename(index={'old':'new'}, inplace=True)\n",
    "\n",
    "# if indexes are not alinged or you need to re-assign indexes\n",
    "# dfLedger.reindex(index=range(len(dfLedger)), method=\"bfill\")\n",
    "# since we merged two dataframes earlier, re-index will not work\n",
    "# until we re-index one of merged dataframes in previous section\n",
    "dfLedger[dfLedger.index.duplicated()]\n",
    "dfLedger.loc[5, \"LEDGER\"]\n",
    "dfLedger.at[4, \"POSTED_TOTAL\"]\n",
    "dfLedger.iat[4, dfLedger.columns.get_loc('POSTED_TOTAL')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas to load data into DataFrames\n",
    "---\n",
    "\n",
    "in this section, we will load real life data from Data sources into data frame and in cases where detail data is reqiured, we will create synthetic data to simulate a real life exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible to read into DataFrame:  accounts.csv\n",
      "Eligible to read into DataFrame:  dept.csv\n",
      "Eligible to read into DataFrame:  ledger.csv\n",
      "Eligible to read into DataFrame:  location.csv\n",
      "(35, 8) (15, 8) (12, 6) (200000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEDGER</th>\n",
       "      <th>ORG</th>\n",
       "      <th>FISCAL_YEAR</th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>ACCOUNT</th>\n",
       "      <th>DEPT</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>POSTED_TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16322</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>39000</td>\n",
       "      <td>2400</td>\n",
       "      <td>20</td>\n",
       "      <td>919779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41713</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>27000</td>\n",
       "      <td>1800</td>\n",
       "      <td>22</td>\n",
       "      <td>282452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94363</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>44000</td>\n",
       "      <td>2000</td>\n",
       "      <td>16</td>\n",
       "      <td>622392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179527</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>35000</td>\n",
       "      <td>1300</td>\n",
       "      <td>21</td>\n",
       "      <td>739302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176177</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>39000</td>\n",
       "      <td>1200</td>\n",
       "      <td>17</td>\n",
       "      <td>573450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37782</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>23000</td>\n",
       "      <td>1300</td>\n",
       "      <td>22</td>\n",
       "      <td>51061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108868</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>38000</td>\n",
       "      <td>1300</td>\n",
       "      <td>18</td>\n",
       "      <td>500272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112988</th>\n",
       "      <td>BUDGET</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>17000</td>\n",
       "      <td>1700</td>\n",
       "      <td>16</td>\n",
       "      <td>102737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39437</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>15000</td>\n",
       "      <td>1000</td>\n",
       "      <td>13</td>\n",
       "      <td>923997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72200</th>\n",
       "      <td>ACTUALS</td>\n",
       "      <td>ABC Inc.</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>42000</td>\n",
       "      <td>2400</td>\n",
       "      <td>22</td>\n",
       "      <td>417441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LEDGER       ORG  FISCAL_YEAR  PERIOD  ACCOUNT  DEPT  LOCATION  \\\n",
       "16322   ACTUALS  ABC Inc.         2022       5    39000  2400        20   \n",
       "41713   ACTUALS  ABC Inc.         2021      10    27000  1800        22   \n",
       "94363   ACTUALS  ABC Inc.         2020       5    44000  2000        16   \n",
       "179527   BUDGET  ABC Inc.         2020       7    35000  1300        21   \n",
       "176177   BUDGET  ABC Inc.         2022      12    39000  1200        17   \n",
       "37782   ACTUALS  ABC Inc.         2022      11    23000  1300        22   \n",
       "108868   BUDGET  ABC Inc.         2021       3    38000  1300        18   \n",
       "112988   BUDGET  ABC Inc.         2022       7    17000  1700        16   \n",
       "39437   ACTUALS  ABC Inc.         2021      11    15000  1000        13   \n",
       "72200   ACTUALS  ABC Inc.         2023       2    42000  2400        22   \n",
       "\n",
       "        POSTED_TOTAL  \n",
       "16322         919779  \n",
       "41713         282452  \n",
       "94363         622392  \n",
       "179527        739302  \n",
       "176177        573450  \n",
       "37782          51061  \n",
       "108868        500272  \n",
       "112988        102737  \n",
       "39437         923997  \n",
       "72200         417441  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dirPath = \"../../../downloads/\"\n",
    "# print(os.listdir(dirPath))\n",
    "\n",
    "# as you can see, there are many files,\n",
    "# we will focus on loading only csv/xls/xlsx files for now\n",
    "\n",
    "for filename in os.listdir(dirPath):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(\"Eligible to read into DataFrame: \", filename)\n",
    "\n",
    "import pandas as pd\n",
    "dfAccounts = pd.read_csv(dirPath+\"accounts.csv\")\n",
    "dfDept = pd.read_csv(dirPath+\"dept.csv\")\n",
    "dfLocation = pd.read_csv(dirPath+\"location.csv\")\n",
    "dfLedger = pd.read_csv(dirPath+\"ledger.csv\")\n",
    "\n",
    "print(dfAccounts.shape, dfDept.shape, dfLocation.shape, dfLedger.shape)\n",
    "dfLedger.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Synthetic Data from real samples\n",
    "\n",
    "**PS:** Just wanted to give you a heads up that in DSPy, a programming language we'll be using for LLMs and RAGs in Finance, there's a nifty tool called `Synthesizer` that can [create synthetic data](https://github.com/AmitXShukla/RPA/blob/main/notebooks/Python%20-%20create%20synthetic%20data%20using%20DSPy%20Synthesizer.ipynb) based on the input data you give it.\n",
    "\n",
    "We'll be using this tool later on, but for now, we'll stick to creating synthetic data using simple scripts.\n",
    "\n",
    "Below is a sample script which creates synthetic data.\n",
    "\n",
    "This script aims to create a dataset with a purposeful bias during specific periods, \n",
    "fostering an environment for both exploratory data analysis and machine learning algorithms\n",
    "to detect and leverage these trends. \n",
    "\n",
    "The objective is to enhance prediction accuracy.\n",
    "By introducing a certain bias, we avoid generating a random and ultimately unhelpful dataset.\n",
    "This method assumes real-life datasets inherently contain hidden patterns\n",
    "that can be revealed through careful analysis and prediction.\n",
    "\n",
    "This exercise's main goal is to uncover such patterns, learn from the data, and train and predict based on these insights.\n",
    "For example, imagine a real-world dataset biased such that every second quarter has poor performance, \n",
    "while each subsequent quarter improves on the previous one.\n",
    "\n",
    "In a real-world scenario, this dataset originates from production ERP (Enterprise Resource Planning) systems,\n",
    "which encompass various aspects of an organization,\n",
    "including HR, customer relationship management, supply chain, inventory, revenue, and financial operations.\n",
    "This dataset is derived from actual production ERP systems,\n",
    "which consolidate data from diverse functional areas like HR, customer management,\n",
    "supply chain, inventory, revenue, and finance within an organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(in millions, except per share data)', '(unaudited)', 'The accompanying notes are an integral part of these consolidated financial statements.', 'Table of Contents', 'Tesla, Inc.', '(in millions, except per share data)', '(unaudited)', 'Automotive sales', 'Automotive leasing', 'Energy generation and storage', 'Services and other', 'Automotive sales', 'Automotive leasing', 'Energy generation and storage', 'Services and other', 'Net income', 'Basic', 'Diluted', 'Basic', 'Diluted', 'The accompanying notes are an integral part of these consolidated financial statements.', 'Table of Contents', 'Tesla, Inc.', '(unaudited)', 'Net income', 'The accompanying notes are an integral part of these consolidated financial statements.', 'Table of Contents', 'Tesla, Inc.', '(in millions, except per share data)', '(unaudited)', 'Exercises of conversion feature of convertible senior notes', 'Issuance of common stock for equity incentive awards', 'Stock-based compensation', 'Distributions to noncontrolling interests', 'Buy-outs of noncontrolling interests', 'Net income', 'Other comprehensive loss', 'Balance as of September 30, 2023', 'Exercises of conversion feature of convertible senior notes', 'Issuance of common stock for equity incentive awards', 'Stock-based compensation', 'Distributions to noncontrolling interests', 'Buy-outs of noncontrolling interests', 'Net (loss) income', 'Other comprehensive loss', 'Balance as of September 30, 2023', 'Table of Contents', 'Exercises of conversion feature of convertible senior notes', 'Issuance of common stock for equity incentive awards', 'Stock-based compensation', 'Distributions to noncontrolling interests', 'Net income', 'Other comprehensive loss', 'Balance as of September 30, 2022', 'Exercises of conversion feature of convertible senior notes', 'Issuance of common stock for equity incentive awards', 'Stock-based compensation', 'Distributions to noncontrolling interests', 'Net (loss) income', 'Other comprehensive loss', 'Balance as of September 30, 2022', 'The accompanying notes are an integral part of these consolidated financial statements.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df_temp = pd.ExcelFile(\"../SampleData/TSLA_Fin_Statements.xlsx\")\n",
    "# df_temp.sheet_names\n",
    "\n",
    "df = pd.read_excel(\"../SampleData/TSLA_Fin_Statements.xlsx\", sheet_name=\"balancesheet\")\n",
    "dfAccountGrp = df[df[df.columns[0]].notnull()].iloc[:,0:1] # remove NaN\n",
    "dfAccountGrp = dfAccountGrp[dfAccountGrp.duplicated(keep=False)] # remove dups\n",
    "print(dfAccountGrp[dfAccountGrp.columns[0]].tolist())\n",
    "# as you can see, it didn't do a good job at creating actual account names,\n",
    "# another trick you can use is,\n",
    "# run this list through a GPT LLM and retrieve list of real account names and eliminate garbage\n",
    "# example prompt:\n",
    "# Could you identify the values in the list that seem to resemble genuine account descriptions, please? Here's the list: [...].\n",
    "\n",
    "########################################\n",
    "# here is the list created by ChatGPT ##\n",
    "########################################\n",
    "# Automoative sales\n",
    "# Automoative leasing\n",
    "# Energy generation and storage\n",
    "# Services and other\n",
    "# Net income\n",
    "# Basic\n",
    "# Diluted\n",
    "# Exercises of conversion feature of convertible senior notes\n",
    "# Issuance of common stock for equity incentive awards\n",
    "# Stock-based compensation\n",
    "# Distributions to noncontrolling interests\n",
    "# Buy-outs of noncontrolling interests\n",
    "# Other comprehensive loss\n",
    "# Balance as of September 30, 2023\n",
    "# Balance as of September 30, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this script to create synthetic Finance, Supply chain dataset\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dirPath = \"../../../downloads/\" # directory where sample csv are generated\n",
    "sampleSize = 100_000 # generate 100k sample rows\n",
    "\n",
    "########################\n",
    "## LOCATION DataFrame ##\n",
    "########################\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "location = pd.DataFrame({\n",
    "    \"ID\":  list(range(11, 23)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n",
    "                     \"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n",
    "                     \"Dallas\",\"San Francisco\"],\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\"] * 3,\n",
    "    \"TYPE\" : \"Physical\",\n",
    "    \"CATEGORY\" : [\"Ship\",\"Recv\",\"Mfg\"] * 4\n",
    "})\n",
    "location.head()\n",
    "\n",
    "########################\n",
    "## ACCOUNTS DataFrame ##\n",
    "########################\n",
    "\n",
    "accounts = pd.DataFrame({\n",
    "    \"ID\":  list(range(10000, 45000, 1000)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Operating Expenses\",\"Non Operating Expenses\",\"Assets\",\n",
    "                     \"Liabilities\",\"Net worth accounts\", \"Statistical Accounts\",\n",
    "                     \"Revenue\"] * 5,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\", \"Region E\"] * 7,\n",
    "    \"TYPE\" : [\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \n",
    "                        \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\n",
    "                        \"REVENUE\"] * 5,\n",
    "    \"CATEGORY\" : [\n",
    "       \t\t\"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n",
    "       \t\t\"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n",
    "       \t\t\"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n",
    "       \t\t\"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n",
    "       \t\t\"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n",
    "       \t\t\"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n",
    "       \t\t\"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"\n",
    "       \t],\n",
    "})\n",
    "accounts.head()\n",
    "\n",
    "##########################\n",
    "## DEPARTMENT DataFrame ##\n",
    "##########################\n",
    "\n",
    "dept = pd.DataFrame({\n",
    "    \"ID\":  list(range(1000, 2500, 100)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"DESCRIPTION\" : [\"Sales & Marketing\",\"Human Resource\",\n",
    "                     \"Information Technology\",\"Business leaders\",\"other temp\"] * 3,\n",
    "    \"REGION\": [\"Region A\",\"Region B\",\"Region C\"] * 5,\n",
    "    \"STATUS\" : \"Active\",\n",
    "    \"CLASSIFICATION\" : [\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"] * 3,\n",
    "    \"TYPE\" : [\"S\",\"H\",\"I\",\"B\",\"O\"] * 3,\n",
    "    \"CATEGORY\" : [\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"] * 3,\n",
    "})\n",
    "dept.head()\n",
    "\n",
    "######################\n",
    "## LEDGER DataFrame ##\n",
    "######################\n",
    "\n",
    "org = \"ABC Inc.\"\n",
    "ledger_type = \"ACTUALS\" # BUDGET, STATS are other Ledger types\n",
    "fiscal_year_from = 2020\n",
    "fiscal_year_to = 2023\n",
    "random.seed(123)\n",
    "\n",
    "ledger = pd.DataFrame({\n",
    "\t\"LEDGER\" : ledger_type,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, \n",
    "                                          fiscal_year_to+1, 1)),k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledger.sample(5)\n",
    "\n",
    "ledger_type = \"BUDGET\" # ACTUALS, STATS are other Ledger types\n",
    "\n",
    "ledgerBudget = pd.DataFrame({\n",
    "\t\"LEDGER\" : ledger_type,\n",
    "\t\"ORG\" : org,\n",
    "\t\"FISCAL_YEAR\": random.choices(list(range(fiscal_year_from, fiscal_year_to+1, 1))\n",
    "                               ,k=sampleSize),\n",
    "\t\"PERIOD\": random.choices(list(range(1, 12+1, 1)),k=sampleSize),\n",
    "\t\"ACCOUNT\" : random.choices(accounts[\"ID\"], k=sampleSize),\n",
    "\t\"DEPT\" : random.choices(dept[\"ID\"], k=sampleSize),\n",
    "\t\"LOCATION\" : random.choices(location[\"ID\"], k=sampleSize),\n",
    "\t\"POSTED_TOTAL\": random.sample(range(1000000), sampleSize)\n",
    "})\n",
    "ledgerBudget.sample(5)\n",
    "\n",
    "#########################################\n",
    "# combined ledger for Actuals and Budget\n",
    "#########################################\n",
    "dfLedger = pd.concat([ledger, ledgerBudget])\n",
    "dfLedger.sample(5)\n",
    "\n",
    "# location.to_csv(f\"{dirPath}location.csv\")\n",
    "# dept.to_csv(f\"{dirPath}dept.csv\")\n",
    "# accounts.to_csv(f\"{dirPath}accounts.csv\")\n",
    "# dfLedger.to_csv(f\"{dirPath}ledger.csv\")\n",
    "\n",
    "print(os.listdir(dirPath))\n",
    "dfLedger.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining data\n",
    "\n",
    "merge, join, concatenate and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## data sorting ####################\n",
    "####################################\n",
    "# iteraion\n",
    "# sorting\n",
    "# items\n",
    "# Vectorized string methods\n",
    "\n",
    "####################################\n",
    "## data operations #################\n",
    "## add(), sub(), mul(), div(), radd(), rsub()\n",
    "####################################\n",
    "\n",
    "####################################\n",
    "## data copy, transformation #######\n",
    "####################################\n",
    "# Tablewise Function Application: pipe()\n",
    "# Row or Column-wise Function Application: apply()\n",
    "# Aggregation API: agg() and transform()\n",
    "# Applying Elementwise Functions: map()\n",
    "\n",
    "####################################\n",
    "# data joins\n",
    "####################################\n",
    "# Aligning objects with each other with align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshaping & pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparse data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timeseries, timedelta and date operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chart visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data analysis | statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data ERD diagram with animation (using manim)\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization using Matplotlib\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization using PlotLy\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization using Seaborn\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX\n",
    "---\n",
    "\n",
    "TODO:\n",
    "\n",
    "In the initial stage of Data Discovery, the primary step involves recognizing and establishing a dynamic repository that encompasses all accessible datasets. It is imperative to identify the relationships between these datasets before embarking on data transformation or analytics.\n",
    "\n",
    "This phase is of utmost importance, as it entails creating an official diagram reminiscent of an Entity-Relationship Diagram (ERD). The crucial tasks include pinpointing data types and discerning the fields that contain valuable information. This not only aids in comprehending the data but also facilitates a deeper understanding of the business processes or the insights derived from these datasets.\n",
    "\n",
    "In this section, we will delve into the Data Discovery phase. We will initiate the process by scrutinizing the available data and crafting an ERD that encapsulates the dataset structures.\n",
    "\n",
    "Let's begin by examining the available dataset.\n",
    "\n",
    "For now, we won't concern ourselves with its source, I'll provide the scripts used to generate it later. \n",
    "\n",
    "Our goal is to simulate a real-world project scenario where analysts often receive unfamiliar datasets and initiate data exploration.\n",
    "\n",
    "The following steps demonstrate this process, and we'll take it one step at a time to learn how to approach data discovery. \n",
    "\n",
    "Keep in mind that there's no one-size-fits-all approach, it varies based on data types and quality. \n",
    "\n",
    "Consider these steps as general guidelines. Let's begin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
