{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ultimate Guide to Mastering the Python Polars DataFrame\n",
    "---\n",
    "The aim of this comprehensive user guide is to equip you with all the necessary knowledge and skills required to utilize Python Polars DataFrames effectively for financial and supply chain data science analytics. \n",
    "\n",
    "It provides an in-depth overview of the most commonly used functions and capabilities of the package.\n",
    "\n",
    "\n",
    "## Table of content\n",
    "---\n",
    "\n",
    "- Why another DataFrame\n",
    "- Installation\n",
    "- Creating DataFrame\n",
    "- Lazy API\n",
    "- IO\n",
    "- Data Types\n",
    "- About Expressions & Contexts\n",
    "    - select\n",
    "    - group by\n",
    "- Polar SQLs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why another DataFrame\n",
    "---\n",
    "Despite the numerous state-of-the-art dataframe packages available in the market, the Polar dataframe, which is built on RUST, boasts the fastest execution speed, enabling it to handle complex data science operations on tabular datasets.\n",
    "\n",
    "- Execution on larger-than-memory (RAM) data analytics\n",
    "- Lazy API vs Eager execution\n",
    "- Automatic Optimization\n",
    "- Embarrassingly Parallel\n",
    "- Easy to learn consistent, predictable API that has strict schema\n",
    "- SQLs like expressions\n",
    "\n",
    "#### Efficient Execution of Analytics on Large-than-Memory (RAM) Data\n",
    "\n",
    "RAM is not a big deal these days as most computers and VMs offer inexpensive GBs of RAM. In fact, the availability of affordable RAM is the primary reason why Pandas-like DataFrames remain the go-to choice, and it is unlikely that Pandas or R Tables will become obsolete anytime soon.\n",
    "\n",
    "However, Polars DataFrames are increasingly gaining popularity among developers due to their ability to harness the horsepower of Apache Spark, the backend support of DuckDB and Apache Arrow, and the ease-of-use of Pandas-like data frame functionalities. \n",
    "\n",
    "Additionally, Polars comes with built-in multi-core, multi-threaded parallel processing, making it a highly preferred choice.\n",
    "\n",
    "#### Lazy API vs Eager execution\n",
    "\n",
    "Just because an API is referred to as \"lazy\" does not necessarily imply that there will be a delay in processing or execution, and conversely, \"eager\" execution doesn't necessarily mean that the programming language will process data transformations or begin execution immediately and more quickly.\n",
    "\n",
    "In simpler terms, using a Lazy API implies that the API will first take the time to optimize the query before execution, which often results in improved performance.\n",
    "\n",
    "To illustrate this concept, consider running SQL on an RDBMS database. If the statistics, indexes, and data partitions have been appropriately optimized and the SQL is written in an optimized manner that utilizes the available statistics, indexes, and data partitions, the results will be delivered more quickly.\n",
    "\n",
    "#### Automatic Optimization\n",
    "\n",
    "We will learn few automation techniques to efficiently optimize queries.\n",
    "\n",
    "#### Embarrassingly Parallel\n",
    "\n",
    "#### Easy to learn consistent, predictable API that has strict schema\n",
    "\n",
    "#### SQLs like expressions\n",
    "\n",
    "## Installation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Polars DataFrame\n",
    "---\n",
    "\n",
    "As stated above, since our objective is learn Data Science operations on Finance and Supply chain dataset, we will focus on creating\n",
    "few real life examples which are similar to Finance and Supply chain.\n",
    "\n",
    "For more information, please learn more about [Finance and Supply chain ERP data](https://amitxshukla.github.io/GeneralLedger.jl/tutorials/erd/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "## Series and DataFrames\n",
    "#########################\n",
    "import polars as pl\n",
    "\n",
    "# with a tuple\n",
    "location_1 = pl.Series([\"CA\", \"OR\", \"WA\", \"TX\", \"NY\"]) \n",
    "# location_1 series when will converted to DataFrame will not have a column name\n",
    "\n",
    "location_2 = pl.Series(\"location\", [\"CA\", \"OR\", \"WA\", \"TX\", \"NY\"])\n",
    "\n",
    "print(f\"Location Type: Series 1: \", location_1)\n",
    "print(f\"Location Type: Series 2: \", location_2)\n",
    "\n",
    "location_1_df = pl.DataFrame(location_1)\n",
    "location_2_df = pl.DataFrame(location_2)\n",
    "print(f\"Location Type: DataFrame 1: \", location_1_df)\n",
    "print(f\"Location Type: DataFrame 2: \", location_2_df)\n",
    "# type(location_1_df[\"location\"]) # will error out, because location_1 series didn't had column name\n",
    "type(location_2_df[\"location\"]), type(location_1), type(location_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (12, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>AS_OF_DATE</th><th>Description</th><th>Region</th><th>Location_Type</th><th>Location_Category</th></tr><tr><td>i64</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>11</td><td>2022-01-01 00:00:00</td><td>&quot;Boston&quot;</td><td>&quot;Region A&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Ship&quot;</td></tr><tr><td>12</td><td>2022-01-01 00:00:00</td><td>&quot;New York&quot;</td><td>&quot;Region B&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Recv&quot;</td></tr><tr><td>13</td><td>2022-01-01 00:00:00</td><td>&quot;Philadelphia&quot;</td><td>&quot;Region C&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Mfg&quot;</td></tr><tr><td>14</td><td>2022-01-01 00:00:00</td><td>&quot;Cleveland&quot;</td><td>&quot;Region D&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Ship&quot;</td></tr><tr><td>15</td><td>2022-01-01 00:00:00</td><td>&quot;Richmond&quot;</td><td>&quot;Region A&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Recv&quot;</td></tr><tr><td>16</td><td>2022-01-01 00:00:00</td><td>&quot;Atlanta&quot;</td><td>&quot;Region B&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Mfg&quot;</td></tr><tr><td>17</td><td>2022-01-01 00:00:00</td><td>&quot;Chicago&quot;</td><td>&quot;Region C&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Ship&quot;</td></tr><tr><td>18</td><td>2022-01-01 00:00:00</td><td>&quot;St. Louis&quot;</td><td>&quot;Region D&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Recv&quot;</td></tr><tr><td>19</td><td>2022-01-01 00:00:00</td><td>&quot;Minneapolis&quot;</td><td>&quot;Region A&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Mfg&quot;</td></tr><tr><td>20</td><td>2022-01-01 00:00:00</td><td>&quot;Kansas City&quot;</td><td>&quot;Region B&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Ship&quot;</td></tr><tr><td>21</td><td>2022-01-01 00:00:00</td><td>&quot;Dallas&quot;</td><td>&quot;Region C&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Recv&quot;</td></tr><tr><td>22</td><td>2022-01-01 00:00:00</td><td>&quot;San Francisco&quot;</td><td>&quot;Region D&quot;</td><td>&quot;Physical&quot;</td><td>&quot;Mfg&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (12, 6)\n",
       "┌─────┬─────────────────────┬───────────────┬──────────┬───────────────┬───────────────────┐\n",
       "│ ID  ┆ AS_OF_DATE          ┆ Description   ┆ Region   ┆ Location_Type ┆ Location_Category │\n",
       "│ --- ┆ ---                 ┆ ---           ┆ ---      ┆ ---           ┆ ---               │\n",
       "│ i64 ┆ datetime[μs]        ┆ str           ┆ str      ┆ str           ┆ str               │\n",
       "╞═════╪═════════════════════╪═══════════════╪══════════╪═══════════════╪═══════════════════╡\n",
       "│ 11  ┆ 2022-01-01 00:00:00 ┆ Boston        ┆ Region A ┆ Physical      ┆ Ship              │\n",
       "│ 12  ┆ 2022-01-01 00:00:00 ┆ New York      ┆ Region B ┆ Physical      ┆ Recv              │\n",
       "│ 13  ┆ 2022-01-01 00:00:00 ┆ Philadelphia  ┆ Region C ┆ Physical      ┆ Mfg               │\n",
       "│ 14  ┆ 2022-01-01 00:00:00 ┆ Cleveland     ┆ Region D ┆ Physical      ┆ Ship              │\n",
       "│ …   ┆ …                   ┆ …             ┆ …        ┆ …             ┆ …                 │\n",
       "│ 19  ┆ 2022-01-01 00:00:00 ┆ Minneapolis   ┆ Region A ┆ Physical      ┆ Mfg               │\n",
       "│ 20  ┆ 2022-01-01 00:00:00 ┆ Kansas City   ┆ Region B ┆ Physical      ┆ Ship              │\n",
       "│ 21  ┆ 2022-01-01 00:00:00 ┆ Dallas        ┆ Region C ┆ Physical      ┆ Recv              │\n",
       "│ 22  ┆ 2022-01-01 00:00:00 ┆ San Francisco ┆ Region D ┆ Physical      ┆ Mfg               │\n",
       "└─────┴─────────────────────┴───────────────┴──────────┴───────────────┴───────────────────┘"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating DataFrame from a dict or a collection of dicts.\n",
    "# let's create a more sophisticated DataFrame\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "location = pl.DataFrame({\n",
    "    \"ID\":  list(range(11, 23)),\n",
    "    \"AS_OF_DATE\" : datetime(2022, 1, 1),\n",
    "    \"Description\" : [\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\", \"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\", \"Dallas\",\"San Francisco\"],\n",
    "    \"Region\": [\"Region A\",\"Region B\",\"Region C\",\"Region D\"] * 3,\n",
    "    \"Location_Type\" : \"Physical\",\n",
    "    \"Location_Category\" : [\"Ship\",\"Recv\",\"Mfg\"] * 4\n",
    "})\n",
    "location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to use Lazy API\n",
    "\n",
    "---\n",
    "\n",
    "In the ideal case we use the lazy API right from a file as the query optimizer may help us to reduce the amount of data we read from the file.\n",
    "- scan_csv\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q1 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    ")\n",
    "```\n",
    "\n",
    "If we were to run the code above on the Reddit CSV the query would not be evaluated. Instead Polars takes each line of code, adds it to the internal query graph and optimizes the query graph.\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q4 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .collect()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution on larger-than-memory (RAM) data analytics\n",
    "\n",
    "---\n",
    "If your data requires more memory than you have available Polars may be able to process the data in batches using streaming mode. To use streaming mode you simply pass the streaming=True argument to collect\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q5 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "```\n",
    "\n",
    "Execution on a partial dataset\n",
    "\n",
    "While you're writing, optimizing or checking your query on a large dataset, querying all available data may lead to a slow development process.\n",
    "\n",
    "You can instead execute the query with the .fetch method. The .fetch method takes a parameter n_rows and tries to 'fetch' that number of rows at the data source. The number of rows cannot be guaranteed, however, as the lazy API does not count how many rows there are at each stage of the query.\n",
    "\n",
    "Here we \"fetch\" 100 rows from the source file and apply the predicates.\n",
    "\n",
    "```\n",
    "import polars as pl\n",
    "\n",
    "from ..paths import DATA_DIR\n",
    "\n",
    "q9 = (\n",
    "    pl.scan_csv(f\"{DATA_DIR}/reddit.csv\")\n",
    "    .with_columns(pl.col(\"name\").str.to_uppercase())\n",
    "    .filter(pl.col(\"comment_karma\") > 0)\n",
    "    .fetch(n_rows=int(100))\n",
    ")\n",
    "```\n",
    "\n",
    "- TODO: cover streaming topic\n",
    "- TODO: cover sinking to a a file\n",
    "- TODO: all topics from Lazy API Chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Optimization\n",
    "---\n",
    "\n",
    "`df.describe_optimized_plan()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RPA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
