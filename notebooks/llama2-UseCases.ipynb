{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama use cases\n",
    "---\n",
    "\n",
    "    Author: Amit Shukla\n",
    "\n",
    "[https://github.com/AmitXShukla](https://github.com/AmitXShukla)\n",
    "\n",
    "[https://twitter.com/ashuklax](https://github.com/AShuklaX)\n",
    "\n",
    "[https://youtube.com/@Amit.Shukla](https://youtube.com/@Amit.Shukla)\n",
    "\n",
    "\n",
    "Meta has recently released Llama 2, a large language model trained with up to 70B parameters, positioning it as the fastest and most advanced solution available. This model is expected to outperform other tools in terms of both speed and accuracy.\n",
    "\n",
    "In this blog post, I will demonstrate some automation use cases I have been working on. \n",
    "\n",
    "It's important to note that these use cases/models will work best when trained on \"in-house\" data. However, training such models is a rigorous task that requires significant computing hours and resources.\n",
    "\n",
    "To make things more accessible and easier to utilize in production, using \"off the shelf\", language models like ChatGPT and Llama 2 is a viable solution.\n",
    "\n",
    "Below, I present some examples of use cases I've been working on. \n",
    "\n",
    "*While these examples are not meant for production*, they still showcase the powerful capabilities of the language models.\n",
    "\n",
    "`Upon completing this blog, you will acquire the skills to build Llama 2 and ChatGPT APIs and harness the capabilities of large language models for practical data analytics tasks.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "---\n",
    "\n",
    "- Introduction\n",
    "- [Llama 2 Installation Windows/Linux](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-UseCases.ipynb)\n",
    "- [Efficient Time and Expense Monitoring with Llama 2](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-Efficient%20Time%20and%20Expense%20Monitoring%20with%20Llama%202.ipynb)\n",
    "- [Using Llama 2 as OCR Vision AI](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-Using%20Llama%202%20as%20OCR%20Vision%20AI.ipynb)\n",
    "- [Llama 2 as Supply Chain Assistant](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-as%20Supply%20Chain%20assistant.ipynb)\n",
    "    - Streamlining 3-Way Receipt Match and Duplicate Voucher Invoices with Llama 2\n",
    "    - Enhancing Fraud Detection: Utilizing Llama 2 as an Advanced Alert System for Monitoring Transactions\n",
    "    - Maximizing Tax Savings, Ensuring Compliance, and Streamlining Audits with Llama 2\n",
    "-  `**WIP**`: Tax Analytics | Spend Classification | Contracts management\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About me\n",
    "I'm Amit Shukla, and I specialize in training neural networks for Finance Supply Chain analysis, enabling them to identify data patterns and make accurate predictions.\n",
    "\n",
    "During the challenges posed by the COVID-19 pandemic, I successfully trained GL and Supply Chain neural networks to anticipate supply chain shortages. The valuable insights gained from this effort have significantly influenced the content of this tutorial series.\n",
    "\t\n",
    "#### Objective:\n",
    "By delving into this powerful tool, we will master the fundamental techniques of utilizing large language models to predict hazards. \n",
    "This knowledge is crucial in preparing finance and supply chain data for advanced analytics, visualization, and predictive modeling using neural networks and machine learning.\n",
    "\t\n",
    "#### Subject\n",
    "It is crucial to emphasize that this specific series will focus exclusively on presenting `production-like examples that demonstrate certain use cases`. It is not intended for production applications. \n",
    "\n",
    "Nevertheless, these examples illustrate highly potent techniques that have practical applications in real-world Data Analytics.\n",
    "\t\n",
    "#### Following\n",
    "In future installments, we will explore Data Analytics and delve into the realm of Data Analytics and machine learning for predictive analytics.\n",
    "\n",
    "Thank you for joining me, and I'm excited to embark on this educational journey together.\n",
    "\t\n",
    "Let's get started.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous video, I demonstrated the process of activating the Open ChatGPT and Llama environments. \n",
    "\n",
    "In this section, I will guide you through the steps to install Llama 2 on a Windows operating system. \n",
    "\n",
    "While the installation process is quite similar to that on Linux, there are a few minor changes that need to be considered. \n",
    "\n",
    "Let's get started!\n",
    "\n",
    "- Step 1: `download miniconda windows installer` [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)\n",
    "- Step 2: create a new conda environment (say llamaConda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before you setup your machine for llama 2, check if you have cuda on your machine\n",
    "\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "# if you don't have cuda and torch on your machine, please move to next step and download cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pytorch cuda\n",
    "# https://pytorch.org/get-started/locally/\n",
    "# uncomment and run this command in Terminal to monitor download progress and debug any error\n",
    "\n",
    "# !conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run these command again and make sure you have cuda available on your machine\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.is_available(),torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this case if you see CUDA out of memory error\n",
    "# also, try to reduce your << --max_batch_size 1 >>, max_split_size_mb:512 and work with \"lowest memory size\" model\n",
    "# !torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b --tokenizer_path tokenizer.model --max_seq_len 512 --max_batch_size 1\n",
    "# clear cache\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:24\"\n",
    "\n",
    "# import gc\n",
    "# del variables\n",
    "# gc.collect()\n",
    "\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "signup and receive model download link from meta.\n",
    "\n",
    "[Mete AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n",
    ">>>\n",
    "    do not use the 'Copy link address' option when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: uncomment and run this command on your machine\n",
    "# make sure, you have a Git installed on your machine if not\n",
    "# for linux run \n",
    "# sudo apt install git-all\n",
    "\n",
    "# for windows, download git from this link\n",
    "# https://git-scm.com/download/win\n",
    "\n",
    "####### clone Meta llama repo ##########\n",
    "git clone https://github.com/facebookresearch/llama.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browse to root of your llama repo\n",
    "\n",
    "## LINUX\n",
    "# cd llama\n",
    "# chmod +x # ./download.sh\n",
    "# ./download.sh\n",
    "\n",
    "## WINDOWS\n",
    "# bash ./download.sh\n",
    "# if this commands error out, download wget.exe from below link and copy wget.ex to C:\\amit.la\\llama\n",
    "# https://eternallybored.org/misc/wget/\n",
    "# make sure, you include << C:\\amit.la\\llama >> to windows environment path so that windows can find it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure, latest conda env is selected as kernel\n",
    "# before activating and installing dependencies\n",
    "\n",
    "!pip install -e .\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for windows\n",
    "# change line #62 on llama/generate.py\n",
    "# from \n",
    "# << torch.distributed.init_process_group(\"gloo|nccl\") >>\n",
    "# to\n",
    "# << torch.distributed.init_process_group(\"gloo|nccl\") >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure, latest conda env (llamaConda) is selected as kernel\n",
    "# before activating and installing dependencies\n",
    "\n",
    "# !pip install -e .\n",
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./example_text_completion.py\n",
    "# ./example_chat_completion.py\n",
    "\n",
    "# change prompts | dialogue \n",
    "#   prompts = [\n",
    "#         # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "#         \"meaning of life is\",\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'torchrun' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# you are ready to use llama\n",
    "!torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b --tokenizer_path tokenizer.model --max_seq_len 512 --max_batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Llama | Cuda errors\n",
    "\n",
    "---\n",
    "\n",
    "Here is another solution proposed by one of users who was able to use Llama on CPU machines.\n",
    "\n",
    "** `Please see, If you successfully install this using this approach, please open an Issue at this GitHub repository so that I can update notes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install https://github.com/krychu/llama instead of https://github.com/facebookresearch/llama\n",
    "\n",
    "# execute download.sh in a new terminal, provide META AI URL and download 7B model and model weights\n",
    "#   run the download.sh script in a terminal, passing the URL provided when prompted to start the download\n",
    "# create a new env\n",
    "\n",
    "# python3 -m venv env\n",
    "# source env/bin/activate\n",
    "\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu #pour la version cpu\n",
    "# python3 -m pip install -e .\n",
    "\n",
    "\n",
    "# torchrun --nproc_per_node 1 example_text_completion.py --ckpt_dir llama-2-7b/ --tokenizer_path tokenizer.model --max_seq_len 128 --max_batch_size 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases\n",
    "---\n",
    "\n",
    "- [Llama 2 Installation Windows/Linux](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-UseCases.ipynb)\n",
    "- [Efficient Time and Expense Monitoring with Llama 2](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-Efficient%20Time%20and%20Expense%20Monitoring%20with%20Llama%202.ipynb)\n",
    "- [Using Llama 2 as OCR Vision AI](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-Using%20Llama%202%20as%20OCR%20Vision%20AI.ipynb)\n",
    "- [Llama 2 as Supply Chain Assistant](https://github.com/AmitXShukla/RPA/blob/main/notebooks/llama2-as%20Supply%20Chain%20assistant.ipynb)\n",
    "    - Streamlining 3-Way Receipt Match and Duplicate Voucher Invoices with Llama 2\n",
    "    - Enhancing Fraud Detection: Utilizing Llama 2 as an Advanced Alert System for Monitoring Transactions\n",
    "    - Maximizing Tax Savings, Ensuring Compliance, and Streamlining Audits with Llama 2\n",
    "-  `**WIP**`: Tax Analytics | Spend Classification | Contracts management\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The use case discussed above exemplify sophisticated business processes and there is certainly lot more which is not covered. \n",
    "\n",
    "This use case merely scratch the surface of what can be achieved with these advanced tools. \n",
    "\n",
    "You may argue that the same results can be attained using simple algebraic mathematics with these datasets, and I fully support and agree with this observation.\n",
    "\n",
    "In essence, the entire field, encompassing Data Science, Python, Llama, and ChatGPT, revolves around uncovering statistical associations within data.\n",
    "\n",
    "However, it is crucial to recognize that the deployment of Llama or ChatGPT-like models does not surpass the importance of traditional statistics,\n",
    "instead, they should be employed to streamline specific tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
