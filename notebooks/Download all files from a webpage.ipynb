{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae17e7d7-d76d-4e5b-829d-22c93c1ea3be",
   "metadata": {},
   "source": [
    "## Download all files from a webpage\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9423601-bd8f-4779-a01a-e0275a46ba1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded example\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = \"https://www.example.com/\"\n",
    "# download all TSLA SEC filings\n",
    "url = \"https://ir.tesla.com/sec-filings\" # doesn't work well\n",
    "# url = \"https://www.sec.gov/edgar/browse/?CIK=1318605&owner=exclude\" # doesn't work well\n",
    "# url=\"https://investor.apple.com/sec-filings/default.aspx\" # doesn't work well\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Create a directory to store the downloaded files\n",
    "os.makedirs(\"SampleData\", exist_ok=True)\n",
    "\n",
    "# Loop through all links on the page\n",
    "for link in soup.find_all(\"a\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href:\n",
    "        # Check if the link is a file\n",
    "        if \".\" in href:\n",
    "            # Construct the full URL for the file\n",
    "            if href.startswith(\"http\"):\n",
    "                file_url = href\n",
    "            else:\n",
    "                file_url = url + href\n",
    "            # Determine the file name and extension\n",
    "            file_name = os.path.basename(file_url)\n",
    "            file_extension = os.path.splitext(file_name)[1]\n",
    "            # Download the file\n",
    "            response = requests.get(file_url)\n",
    "            with open(f\"SampleData/{file_name}\", \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {file_name}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4755c7a-c936-4378-8ac8-b6151eb83fd3",
   "metadata": {},
   "source": [
    "In this code, we first define the URL of the webpage we want to download files from (url) and send a GET request to the URL to obtain the HTML content of the page. We then use Beautiful Soup to parse the HTML and find all links on the page. For each link, we check if it is a file by looking for a \".\" in the link's href attribute. If the link is a file, we construct the full URL for the file (which may be a relative URL on the page) and download the file using the requests.get() function. We save each downloaded file in a \"downloads\" directory in the current working directory.\n",
    "Note that you may need to modify this code depending on the types of files you want to download and their size. Additionally, some websites may have restrictions on downloading files, so you should always check the terms of service and robots.txt file of a website before downloading files from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
